{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1, COMS 4995_005, Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Possible Score 120 Points  (100 + 20 Extra Credits)\n",
    "----\n",
    "\n",
    "# Part1: (Basic Neural Nework) (70 Points)\n",
    "\n",
    "### Part 1.1:\n",
    "\n",
    "- Divide the training data into 80% training set and 20% validation set. \n",
    "- Implement the functions in the ipython notebook so that you can train your network. \n",
    "- Your code should take network structure, training data, hyperparameters and generate validation set accuracy.\n",
    "- Use Relu activation for intermediate layers and use cross entropy loss after taking softmax on the output of the final layer.\n",
    "\n",
    "\n",
    "Plot a graph of loss (plot loss on training set and val set both) vs Iterations (which would be decreasing with probably some noise) - **60 Points**\n",
    "\n",
    "\n",
    "### Part 1.2:\n",
    "\n",
    "Test your model accuracy on test set. If it is more than **47%**, you will get an additional score of **10 points**\n",
    "\n",
    "# Part 2: (Regularization) (30 Points)\n",
    "\n",
    "### Part 2.1 (15 Points) :\n",
    "\n",
    "Modify code to add L2 regularization. Report the validation accuracy.\n",
    "\n",
    "You should get a validation and test accuracy of more than the one reported in Part-1\n",
    "\n",
    "Plot a graph of loss (plot loss on training set and val set both) vs Iterations (which would be decreasing with probably some noise) \n",
    "\n",
    "\n",
    "### Part 2.2 (15 Points):\n",
    "\n",
    "You should get a validation and test accuracy crossing **50%**\n",
    "\n",
    "\n",
    "# Extra Credit (20 Points)\n",
    "\n",
    "Show your excitement on deep learning! Top **3 scorers** will get these **20 points**\n",
    "\n",
    "**(note that you cannot use convolutional layers in this part also)**\n",
    "\n",
    "(Hints) Boost your accuracy by trying out: \n",
    "- Dropout Regularization\n",
    "- Batch Normalization\n",
    "- Other optimizers like Adam\n",
    "- Learning Rate Decay\n",
    "- Data Augmentation \n",
    "- Different Initializations for weights like Xaviers etc.\n",
    "\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Guidelines:\n",
    "\n",
    "1. Write your code in **Python 3**.\n",
    "2. **DONOT** import any other packages.\n",
    "3. Click **https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz** -> download **cifar-10-python.tar.gz** -> extract as **cifar-10-python**\n",
    "4. Ensure that **this ipython notebook** and **cifar-10-python** folder are in the same folder.\n",
    "\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Submission Guidelines:\n",
    "\n",
    "1. Run this ipython notebook once and submit this file. Ensure that the outputs are printed properly. We will first see the outputs, if there are no outputs, we may not run the notebook at all.\n",
    "2. Training on the **test data** is considered cheating. If we find any clue of that happening, then we will disqualify the submission and it will be reported accordingly.\n",
    "3. Each team member needs to separately submit the the file named uni.ipynb on courseworks.\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Information\n",
    "\n",
    "Team Member1 (Name,UNI): Huibo Zhao hz2480\n",
    "\n",
    "Team Member2 (Name, UNI): Tingmeng Li tl2758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Do not import other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of a Fully Connected Network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    You can add more functions in this class, and also modify inputs and outputs of each function\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dim, lambd=0):\n",
    "        \"\"\"\n",
    "        layer_dim: List containing layer dimensions. \n",
    "        \n",
    "        Code:\n",
    "        Initialize weight and biases for each layer\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize weight and biases as dictionaries\n",
    "        self.W = {}\n",
    "        self.b = {}\n",
    "        \n",
    "        # initialize cache (used in backpropagation) as dictionaries\n",
    "        self.cacheAffine = {}\n",
    "        self.cacheRelu = {}\n",
    "    \n",
    "        # other initialization\n",
    "        self.lambd = lambd\n",
    "        self.num_layers = len(layer_dim)\n",
    "        \n",
    "        for i in range(1, self.num_layers):\n",
    "            ### normal distribution for randomalize ###\n",
    "            self.W[i] = 0.01 * np.random.randn(layer_dim[i],layer_dim[i-1]) \n",
    "            self.b[i] = 0.01 * np.random.randn(layer_dim[i],1)\n",
    "\n",
    "    def feedforward(self,X,eval=0):\n",
    "        \"\"\"\n",
    "        Expected Functionality: \n",
    "        Returns output of the neural network for input X. Also returns cache, which contains outputs of \n",
    "        intermediate layers which would be useful during backprop.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # for layers except last layer\n",
    "            # affineForward\n",
    "            # relu_forward\n",
    "    \n",
    "        # last layer\n",
    "            # affineForward\n",
    "            \n",
    "            \n",
    "        temp_input = X.copy()\n",
    "        \n",
    "        \n",
    "        #### when we use feedforward for evaluation, we don't want to update cache! ###\n",
    "        if (eval == 1):\n",
    "            for i in range(1,self.num_layers):\n",
    "                affine_temp_out, no_use = self.affineForward(temp_input,self.W[i],self.b[i])\n",
    "                if (i == self.num_layers-1):\n",
    "                    return affine_temp_out\n",
    "                temp_input = self.relu_forward(affine_temp_out)\n",
    "        #################################################################################\n",
    "        \n",
    "        for i in range(1,self.num_layers):\n",
    "            affine_temp_out, self.cacheAffine[i] = self.affineForward(temp_input, self.W[i],self.b[i])\n",
    "            if (i == self.num_layers-1): # the last layer is output layer\n",
    "                break\n",
    "            temp_input = self.relu_forward(affine_temp_out)\n",
    "            self.cacheRelu[i] = temp_input\n",
    "        \n",
    "        return affine_temp_out\n",
    "            \n",
    "            \n",
    "    def loss_function(self, At, Y):\n",
    "        \"\"\"\n",
    "        At is the output of the last layer, returned by feedforward.\n",
    "        Y contains true labels for this batch.\n",
    "        this function takes softmax the last layer's output and calculates loss.\n",
    "        the gradient of loss with respect to the activations of the last layer are also returned by this function.\n",
    "        \n",
    "        \"\"\"\n",
    "        num_samples = At.shape[1] # number of samples\n",
    "        \n",
    "        softmax_output = np.exp(At - np.average(At,axis=0)) # do exponential to each element\n",
    "                                                                           # subtract average from each\n",
    "                                                                           # element won't affect final result\n",
    "        \n",
    "        temp_sum = np.sum(softmax_output,axis=0,keepdims=True) # sum(e^a[j])\n",
    "        softmax_output = softmax_output / temp_sum # softmax done\n",
    "        \n",
    "        # for computing the gradient of loss with respect to the activations of the last layer\n",
    "        # dC/dh = y_i (prediction) - t_i (target value)\n",
    "        # Therefore, we only need to minus one for those corresponding to the correct target value\n",
    "        \n",
    "        dAt = softmax_output.copy()\n",
    "        dAt [Y,np.arange(num_samples)] -= 1\n",
    "        \n",
    "        # for computing average cost of all samples, we extract true class's softmax value\n",
    "        # and apply -log to them and then sum them up and lastly divided by the total numbers\n",
    "        ave_loss = np.sum(-np.log(softmax_output[Y,np.arange(num_samples)])) / num_samples\n",
    "        \n",
    "        return ave_loss, dAt\n",
    "\n",
    "        # softmax: e^a(i) / (sum(e^a[j]) for j in all classes)\n",
    "        # cross entropy loss:  -log(true class's softmax value(prediction))\n",
    "    \n",
    "        # for part2: when lambd > 0, you need to change the definition of loss accordingly\n",
    "        \n",
    "    def train(self, X, Y, max_iters=5000, batch_size=100, learning_rate=0.01, validate_every=200):\n",
    "        \"\"\"\n",
    "        X: (3072 dimensions, 50000 examples) (Cifar train data)\n",
    "        Y: (1 dimension, 50000 examples)\n",
    "        lambd: the hyperparameter corresponding to L2 regularization\n",
    "        \n",
    "        Divide X, Y into train(80%) and val(20%), during training do evaluation on val set\n",
    "        after every validate_every iterations and in the end use the parameters corresponding to the best\n",
    "        val set to test on the Cifar test set. Print the accuracy that is calculated on the val set during \n",
    "        training. Also print the final test accuracy. Ensure that these printed values can be seen in the .ipynb file you\n",
    "        submit.\n",
    "        \n",
    "        Expected Functionality: \n",
    "        This function will call functions feedforward, backprop and update_params. \n",
    "        Also, evaluate on the validation set for tuning the hyperparameters.\n",
    "        \"\"\"\n",
    "        # generate training data index\n",
    "        index = np.random.choice(50000,50000,replace=False)\n",
    "        train_index = index[0:40000]\n",
    "        val_index = index[40000:50000]\n",
    "        \n",
    "        train_X = X[:,train_index]\n",
    "        train_Y = Y[:,train_index]\n",
    "        \n",
    "        val_X = X[:,val_index]\n",
    "        val_Y = Y[:,val_index]\n",
    "        \n",
    "\n",
    "        # for some iterations\n",
    "            # get current batch\n",
    "            # feedforward\n",
    "            # loss_function\n",
    "            # backprop\n",
    "            # updateParameters\n",
    "        temp_W = self.W.copy()\n",
    "        temp_b = self.W.copy()\n",
    "        best_accuracy = 0\n",
    "        for i in range(max_iters):\n",
    "            batch_X, batch_Y = self.get_batch(train_X,train_Y,batch_size)\n",
    "            \n",
    "            fdforward_output = self.feedforward(batch_X)\n",
    "            \n",
    "            loss, dAt = self.loss_function(fdforward_output,batch_Y)\n",
    "            \n",
    "            \n",
    "            if (i % validate_every == 0):\n",
    "                val_accuracy = self.evaluate(val_X,val_Y)\n",
    "                fdforward_output_val = self.feedforward(val_X,eval=1)\n",
    "                loss_val, no_use = self.loss_function(fdforward_output_val,val_Y)\n",
    "                print(\"loss in iteration\",i,\"is\",loss,\",accuracy on val set is\",val_accuracy,\n",
    "                     \",loss on val set is\",loss_val)\n",
    "                \n",
    "                if val_accuracy > best_accuracy:\n",
    "                    best_accuracy = val_accuracy\n",
    "                    temp_W = self.W.copy()\n",
    "                    temp_b = self.b.copy()\n",
    "                          \n",
    "            gradients = self.backprop(loss,dAt)\n",
    "            \n",
    "            self.updateParameters(gradients,learning_rate)\n",
    "        \n",
    "        self.W = temp_W\n",
    "        self.b = temp_b\n",
    "    \n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "        affine_product = W.dot(A) + b\n",
    "        cache = (A,W)\n",
    "        return affine_product, cache\n",
    "    \n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        Backward pass for the affine layer.\n",
    "        dA_prev: gradient from the next layer.\n",
    "        cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "        \n",
    "        # unpack cache\n",
    "        A = cache[0]\n",
    "        W = cache[1]\n",
    "        \n",
    "        num_samples = A.shape[1] # number of samples\n",
    "        \n",
    "        dA = W.T.dot(dA_prev)\n",
    "        dW = dA_prev.dot(A.T) / num_samples # dW is the average dW for all samples\n",
    "        db = np.mean(dA_prev,axis=1,keepdims=True) # compress each row to be its mean\n",
    "        \n",
    "        \n",
    "        return dA, dW, db\n",
    "        \n",
    "    def relu_forward(self, X):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        Forward pass of relu activation\n",
    "        \n",
    "        returns (A, cache)\n",
    "        \"\"\"\n",
    "        relu_output = np.maximum(0,X)\n",
    "        return relu_output\n",
    "        \n",
    "        \n",
    "    def relu_backward(self, dx, cached_x):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        backward pass for relu activation\n",
    "        \"\"\"\n",
    "        deriv = cached_x.copy()\n",
    "        deriv [deriv > 0] = 1\n",
    "        return dx * deriv # don't use dot product\n",
    "        \n",
    "        \n",
    "    def get_batch(self, X, Y, batch_size):\n",
    "        \"\"\"\n",
    "        Expected Functionality: \n",
    "        given the full training data (X, Y), return batches for each iteration of forward and backward prop.\n",
    "        \"\"\"\n",
    "        ### with or without replacement?###\n",
    "        index = np.random.choice(range(X.shape[1]),batch_size,replace=False)\n",
    "        \n",
    "        return X[:,index], Y[:,index]\n",
    "        \n",
    "    def backprop(self, loss, dAct):\n",
    "        \"\"\"\n",
    "        Expected Functionality: \n",
    "        returns gradients for all parameters in the network.\n",
    "        dAct is the gradient of loss with respect to the output of final layer of the network.\n",
    "        \"\"\"\n",
    "        gradient_W = {}\n",
    "        gradient_b = {}\n",
    "        \n",
    "        dA, dW, db = self.affineBackward(dAct,self.cacheAffine[self.num_layers-1])\n",
    "        gradient_W[self.num_layers-1] = dW\n",
    "        gradient_b[self.num_layers-1] = db\n",
    "        \n",
    "        for i in range(self.num_layers-2,0,-1):\n",
    "            relu_gradient = self.relu_backward(dA,self.cacheRelu[i])\n",
    "            dA,dW,db = self.affineBackward(relu_gradient,self.cacheAffine[i])\n",
    "            gradient_W[i] = dW\n",
    "            gradient_b[i] = db\n",
    "        \n",
    "        return (gradient_W,gradient_b)\n",
    "        # last layer \n",
    "            # affineBackward\n",
    "            # set dW[last layer], db[last layer]\n",
    "            \n",
    "        # for layers except last layer\n",
    "            # relu_Backward\n",
    "            # affineBackward\n",
    "            # set dW[layer], db[layer]\n",
    "    \n",
    "        # for part2: lambd > 0, you need to change the definition accordingly\n",
    "    \n",
    "    def updateParameters(self, gradients, learning_rate, lambd=0):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        use gradients returned by backprop to update the parameters.\n",
    "        \"\"\"\n",
    "        gradient_W = gradients[0]\n",
    "        gradient_b = gradients[1]\n",
    "        \n",
    "        for i in range(1,self.num_layers-1):\n",
    "            self.W[i] -= learning_rate * gradient_W[i]\n",
    "            self.b[i] -= learning_rate * gradient_b[i]\n",
    "        \n",
    "        self.W[self.num_layers-1] -= learning_rate * gradient_W[self.num_layers-1]\n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        '''\n",
    "        X: X_test (3472 dimensions, 10000 examples)\n",
    "        Y: Y_test (1 dimension, 10000 examples)\n",
    "        \n",
    "        Expected Functionality: \n",
    "        print accuracy on test set\n",
    "        '''\n",
    "        fdforward_output = self.feedforward(X_test,eval=1)\n",
    "        prediction = np.argmax(fdforward_output, axis = 0)\n",
    "        count = 0\n",
    "        for i in range(X_test.shape[1]):\n",
    "            if prediction[i] == Y_test[:,i][0]:\n",
    "                count += 1\n",
    "        return count/X_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    \n",
    "    def unpickle(self,file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "    \n",
    "    def load_train_data(self):\n",
    "        '''\n",
    "        loads training data: 50,000 examples with 3072 features\n",
    "        '''\n",
    "        X_train = None\n",
    "        Y_train = None\n",
    "        for i in range(1, 6):\n",
    "            pickleFile = self.unpickle('cifar-10-batches-py/data_batch_{}'.format(i))\n",
    "            dataX = pickleFile[b'data']\n",
    "            dataY = pickleFile[b'labels']\n",
    "            if type(X_train) is np.ndarray:\n",
    "                X_train = np.concatenate((X_train, dataX))\n",
    "                Y_train = np.concatenate((Y_train, dataY))\n",
    "            else:\n",
    "                X_train = dataX\n",
    "                Y_train = dataY\n",
    "\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "\n",
    "        return X_train.T, Y_train.T\n",
    "\n",
    "    def load_test_data(self):\n",
    "        '''\n",
    "        loads testing data: 10,000 examples with 3072 features\n",
    "        '''\n",
    "        X_test = None\n",
    "        Y_test = None\n",
    "        pickleFile = self.unpickle('cifar-10-batches-py/test_batch')\n",
    "        dataX = pickleFile[b'data']\n",
    "        dataY = pickleFile[b'labels']\n",
    "        if type(X_test) is np.ndarray:\n",
    "            X_test = np.concatenate((X_test, dataX))\n",
    "            Y_test = np.concatenate((Y_test, dataY))\n",
    "        else:\n",
    "            X_test = np.array(dataX)\n",
    "            Y_test = np.array(dataY)\n",
    "\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "        return X_test.T, Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train: (3072, 50000) -> 50000 examples, 3072 features\n",
      "Y_Train: (1, 50000) -> 50000 examples, 1 features\n",
      "X_Test: (3072, 10000) -> 10000 examples, 3072 features\n",
      "Y_Test: (1, 10000) -> 10000 examples, 1 features\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train = Loader().load_train_data()\n",
    "X_test, Y_test = Loader().load_test_data()\n",
    "\n",
    "print(\"X_Train: {} -> {} examples, {} features\".format(X_train.shape, X_train.shape[1], X_train.shape[0]))\n",
    "print(\"Y_Train: {} -> {} examples, {} features\".format(Y_train.shape, Y_train.shape[1], Y_train.shape[0]))\n",
    "print(\"X_Test: {} -> {} examples, {} features\".format(X_test.shape, X_test.shape[1], X_test.shape[0]))\n",
    "print(\"Y_Test: {} -> {} examples, {} features\".format(Y_test.shape, Y_test.shape[1], Y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration 0 is 2.30280354601 ,accuracy on val set is 0.1017 ,loss on val set is 2.30468577582\n",
      "loss in iteration 200 is 1.90406137012 ,accuracy on val set is 0.3057 ,loss on val set is 1.91676558457\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-6402237443cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 3072 is the input feature size, 10 is the number of outputs in the final layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mFCN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullyConnectedNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_dimensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mFCN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidate_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# lambd, the L2 regularization penalty hyperparamter will be 0 for this part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFCN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# print accuracy on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-f836fabdca29>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y, max_iters, batch_size, learning_rate, validate_every)\u001b[0m\n\u001b[1;32m    161\u001b[0m                     \u001b[0mtemp_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdAt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateParameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-f836fabdca29>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self, loss, dAct)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0mrelu_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcacheRelu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffineBackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelu_gradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcacheAffine\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m             \u001b[0mgradient_W\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mgradient_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-f836fabdca29>\u001b[0m in \u001b[0;36maffineBackward\u001b[0;34m(self, dA_prev, cache)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mdA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[0;31m# dW is the average dW for all samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA_prev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# compress each row to be its mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_dimensions = [3072,1000,200,50,10]  # including the input and output layers  \n",
    "# 3072 is the input feature size, 10 is the number of outputs in the final layer\n",
    "FCN = FullyConnectedNetwork(layer_dimensions, lambd=0)\n",
    "FCN.train(X_train, Y_train, max_iters=8000, batch_size=200, learning_rate=0.01,validate_every=200)\n",
    "# lambd, the L2 regularization penalty hyperparamter will be 0 for this part\n",
    "y_predicted = FCN.evaluate(X_test,Y_test)  # print accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dimensions = [3072,..,.., 10]  # including the input and output layers  \n",
    "# 3072 is the input feature size, 10 is the number of outputs in the final layer\n",
    "FCN = FullyConnectedNetwork(layer_dimensions, lambd=0.1)\n",
    "FCN.train(X_train, Y_train, max_iters=10000, batch_size=200, learning_rate=0.0001, validate_every=200)\n",
    "# lambd, the L2 regularization penalty hyperparamter will not be 0 for this part\n",
    "y_predicted = FCN.evaluate(X_test)  # print accuracy on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
