{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1, COMS 4995_005, Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Possible Score 120 Points  (100 + 20 Extra Credits)\n",
    "----\n",
    "\n",
    "# Part1: (Basic Neural Nework) (70 Points)\n",
    "\n",
    "### Part 1.1:\n",
    "\n",
    "- Divide the training data into 80% training set and 20% validation set. \n",
    "- Implement the functions in the ipython notebook so that you can train your network. \n",
    "- Your code should take network structure, training data, hyperparameters and generate validation set accuracy.\n",
    "- Use Relu activation for intermediate layers and use cross entropy loss after taking softmax on the output of the final layer.\n",
    "\n",
    "\n",
    "Plot a graph of loss (plot loss on training set and val set both) vs Iterations (which would be decreasing with probably some noise) - **60 Points**\n",
    "\n",
    "\n",
    "### Part 1.2:\n",
    "\n",
    "Test your model accuracy on test set. If it is more than **47%**, you will get an additional score of **10 points**\n",
    "\n",
    "# Part 2: (Regularization) (30 Points)\n",
    "\n",
    "### Part 2.1 (15 Points) :\n",
    "\n",
    "Modify code to add L2 regularization. Report the validation accuracy.\n",
    "\n",
    "You should get a validation and test accuracy of more than the one reported in Part-1\n",
    "\n",
    "Plot a graph of loss (plot loss on training set and val set both) vs Iterations (which would be decreasing with probably some noise) \n",
    "\n",
    "\n",
    "### Part 2.2 (15 Points):\n",
    "\n",
    "You should get a validation and test accuracy crossing **50%**\n",
    "\n",
    "\n",
    "# Extra Credit (20 Points)\n",
    "\n",
    "Show your excitement on deep learning! Top **3 scorers** will get these **20 points**\n",
    "\n",
    "**(note that you cannot use convolutional layers in this part also)**\n",
    "\n",
    "(Hints) Boost your accuracy by trying out: \n",
    "- Dropout Regularization\n",
    "- Batch Normalization\n",
    "- Other optimizers like Adam\n",
    "- Learning Rate Decay\n",
    "- Data Augmentation \n",
    "- Different Initializations for weights like Xaviers etc.\n",
    "\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Guidelines:\n",
    "\n",
    "1. Write your code in **Python 3**.\n",
    "2. **DONOT** import any other packages.\n",
    "3. Click **https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz** -> download **cifar-10-python.tar.gz** -> extract as **cifar-10-python**\n",
    "4. Ensure that **this ipython notebook** and **cifar-10-python** folder are in the same folder.\n",
    "\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Submission Guidelines:\n",
    "\n",
    "1. Run this ipython notebook once and submit this file. Ensure that the outputs are printed properly. We will first see the outputs, if there are no outputs, we may not run the notebook at all.\n",
    "2. Training on the **test data** is considered cheating. If we find any clue of that happening, then we will disqualify the submission and it will be reported accordingly.\n",
    "3. Each team member needs to separately submit the the file named uni.ipynb on courseworks.\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Information\n",
    "\n",
    "Team Member1 (Name,UNI): Huibo Zhao hz2480\n",
    "\n",
    "Team Member2 (Name, UNI): Tingmeng Li tl2758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Do not import other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FullyConnectedNetwork(object):\n",
    "    \"\"\"\n",
    "    Abstraction of a Fully Connected Network.\n",
    "    Stores parameters, activations, cached values. \n",
    "    You can add more functions in this class, and also modify inputs and outputs of each function\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, layer_dim, lambd=0):\n",
    "        \"\"\"\n",
    "        layer_dim: List containing layer dimensions. \n",
    "        \n",
    "        Code:\n",
    "        Initialize weight and biases for each layer\n",
    "        \"\"\"\n",
    "        \n",
    "        # initialize weight and biases as dictionaries\n",
    "        self.W = {}\n",
    "        self.b = {}\n",
    "        \n",
    "        self.cacheAffine = {}\n",
    "        self.cacheRelu = {}\n",
    "    \n",
    "        self.lambd = lambd\n",
    "        \n",
    "        self.num_layers = len(layer_dim)\n",
    "        \n",
    "        for i in range(1, self.num_layers):\n",
    "            ### could try normal distribution for randomalize ###\n",
    "            self.W[i] = 0.01 * np.random.randn(layer_dim[i],layer_dim[i-1]) \n",
    "            self.b[i] = 0.01 * np.random.randn(layer_dim[i],1)\n",
    "            \n",
    "        \n",
    "        \n",
    "    def feedforward(self,X):\n",
    "        \"\"\"\n",
    "        Expected Functionality: \n",
    "        Returns output of the neural network for input X. Also returns cache, which contains outputs of \n",
    "        intermediate layers which would be useful during backprop.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # for layers except last layer\n",
    "            # affineForward\n",
    "            # relu_forward\n",
    "    \n",
    "        # last layer\n",
    "            # affineForward \n",
    "        temp_input = X.copy()\n",
    "        for i in range(1,self.num_layers):\n",
    "            affine_temp_out, self.cacheAffine[i] = self.affineForward(temp_input, self.W[i],self.b[i])\n",
    "            if (i == self.num_layers-1): # the last layer is output layer\n",
    "                break\n",
    "            temp_input = self.relu_forward(affine_temp_out)\n",
    "            self.cacheRelu[i] = temp_input\n",
    "        \n",
    "        return affine_temp_out\n",
    "            \n",
    "            \n",
    "    def loss_function(self, At, Y):\n",
    "        \"\"\"\n",
    "        At is the output of the last layer, returned by feedforward.\n",
    "        Y contains true labels for this batch.\n",
    "        this function takes softmax the last layer's output and calculates loss.\n",
    "        the gradient of loss with respect to the activations of the last layer are also returned by this function.\n",
    "        \n",
    "        \"\"\"\n",
    "        num_samples = At.shape[1] # number of samples\n",
    "        \n",
    "        softmax_output = np.exp(At) # do exponential to each element\n",
    "        \n",
    "        #print(\"At is\",At)\n",
    "        #print(softmax_output.shape)\n",
    "        #print(softmax_output[:,1])\n",
    "\n",
    "        \n",
    "        temp_sum = np.sum(softmax_output,axis=0,keepdims=True) # sum(e^a[j])\n",
    "        softmax_output = softmax_output / temp_sum # softmax done\n",
    "        \n",
    "        #print(softmax_output[:,1])\n",
    "        \n",
    "        # for computing the gradient of loss with respect to the activations of the last layer\n",
    "        # dC/dh = y_i (prediction) - t_i (target value)\n",
    "        # Therefore, we only need to minus one for those corresponding to the correct target value\n",
    "        \n",
    "        dAt = softmax_output.copy()\n",
    "        dAt [Y,np.arange(num_samples)] -= 1\n",
    "        \n",
    "        # for computing average cost of all samples, we extract true class's softmax value\n",
    "        # and apply -log to them and then sum them up and lastly divided by the total numbers\n",
    "        ave_loss = np.sum(-np.log(softmax_output[Y,np.arange(num_samples)])) / num_samples\n",
    "        \n",
    "        return ave_loss, dAt\n",
    "\n",
    "        # softmax: e^a(i) / (sum(e^a[j]) for j in all classes)\n",
    "        # cross entropy loss:  -log(true class's softmax value(prediction))\n",
    "    \n",
    "        # for part2: when lambd > 0, you need to change the definition of loss accordingly\n",
    "        \n",
    "    def train(self, X, Y, max_iters=5000, batch_size=100, learning_rate=0.01, validate_every=200):\n",
    "        \"\"\"\n",
    "        X: (3072 dimensions, 50000 examples) (Cifar train data)\n",
    "        Y: (1 dimension, 50000 examples)\n",
    "        lambd: the hyperparameter corresponding to L2 regularization\n",
    "        \n",
    "        Divide X, Y into train(80%) and val(20%), during training do evaluation on val set\n",
    "        after every validate_every iterations and in the end use the parameters corresponding to the best\n",
    "        val set to test on the Cifar test set. Print the accuracy that is calculated on the val set during \n",
    "        training. Also print the final test accuracy. Ensure that these printed values can be seen in the .ipynb file you\n",
    "        submit.\n",
    "        \n",
    "        Expected Functionality: \n",
    "        This function will call functions feedforward, backprop and update_params. \n",
    "        Also, evaluate on the validation set for tuning the hyperparameters.\n",
    "        \"\"\"\n",
    "        # generate training data index\n",
    "        index = np.random.choice(50000,50000,replace=False)\n",
    "        train_index = index[0:40000]\n",
    "        val_index = index[40000:50000]\n",
    "        \n",
    "        train_X = X[:,train_index]\n",
    "        train_Y = Y[:,train_index]\n",
    "        \n",
    "        val_X = X[:,val_index]\n",
    "        val_Y = Y[:,val_index]\n",
    "        \n",
    "        \n",
    "                \n",
    "        # for some iterations\n",
    "            # get current batch\n",
    "            # feedforward\n",
    "            # loss_function\n",
    "            # backprop\n",
    "            # updateParameters\n",
    "        for i in range(max_iters):\n",
    "            batch_X, batch_Y = self.get_batch(train_X,train_Y,batch_size)\n",
    "            \n",
    "            \n",
    "            fdforward_output = self.feedforward(batch_X)\n",
    "            \n",
    "            #print(fdforward_output)\n",
    "            \n",
    "            loss, dAt = self.loss_function(fdforward_output,batch_Y)\n",
    "            print(\"loss in iteration\",i,\"is\",loss)\n",
    "            \n",
    "            gradients = self.backprop(loss,dAt)\n",
    "            \n",
    "            self.updateParameters(gradients,learning_rate)\n",
    "    \n",
    "    def affineForward(self, A, W, b):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        Forward pass for the affine layer.\n",
    "        :param A: input matrix, shape (L, S), where L is the number of hidden units in the previous layer and S is\n",
    "        the number of samples\n",
    "        :returns: the affine product WA + b, along with the cache required for the backward pass\n",
    "        \"\"\"\n",
    "        affine_product = W.dot(A) + b\n",
    "        cache = (A,W)\n",
    "        return affine_product, cache\n",
    "    \n",
    "    def affineBackward(self, dA_prev, cache):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        Backward pass for the affine layer.\n",
    "        dA_prev: gradient from the next layer.\n",
    "        cache: cache returned in affineForward\n",
    "        :returns dA: gradient on the input to this layer\n",
    "                 dW: gradient on the weights\n",
    "                 db: gradient on the bias\n",
    "        \"\"\"\n",
    "        \n",
    "        # unpack cache\n",
    "        A = cache[0]\n",
    "        W = cache[1]\n",
    "        \n",
    "        num_samples = A.shape[1] # number of samples\n",
    "        \n",
    "        dA = W.T.dot(dA_prev)\n",
    "        dW = dA_prev.dot(A.T) / num_samples # dW is the average dW for all samples\n",
    "        db = np.mean(dA_prev,axis=1,keepdims=True) # compress each row to be its mean\n",
    "        \n",
    "        \n",
    "        return dA, dW, db\n",
    "        \n",
    "    def relu_forward(self, X):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        Forward pass of relu activation\n",
    "        \n",
    "        returns (A, cache)\n",
    "        \"\"\"\n",
    "        relu_output = np.maximum(0,X)\n",
    "        return relu_output\n",
    "        \n",
    "        \n",
    "    def relu_backward(self, dx, cached_x):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        backward pass for relu activation\n",
    "        \"\"\"\n",
    "        deriv = cached_x.copy()\n",
    "        deriv [deriv > 0] = 1\n",
    "        return dx * deriv # don't use dot product\n",
    "        \n",
    "        \n",
    "    def get_batch(self, X, Y, batch_size):\n",
    "        \"\"\"\n",
    "        Expected Functionality: \n",
    "        given the full training data (X, Y), return batches for each iteration of forward and backward prop.\n",
    "        \"\"\"\n",
    "        ### with or without replacement?###\n",
    "        index = np.random.choice(range(X.shape[1]),batch_size,replace=False)\n",
    "        \n",
    "        return X[:,index], Y[:,index]\n",
    "        \n",
    "    def backprop(self, loss, dAct):\n",
    "        \"\"\"\n",
    "        Expected Functionality: \n",
    "        returns gradients for all parameters in the network.\n",
    "        dAct is the gradient of loss with respect to the output of final layer of the network.\n",
    "        \"\"\"\n",
    "        gradient_W = {}\n",
    "        gradient_b = {}\n",
    "        \n",
    "        dA, dW, db = self.affineBackward(dAct,self.cacheAffine[self.num_layers-1])\n",
    "        gradient_W[self.num_layers-1] = dW\n",
    "        gradient_b[self.num_layers-1] = db\n",
    "        \n",
    "        for i in range(self.num_layers-2,0,-1):\n",
    "            relu_gradient = self.relu_backward(dA,self.cacheRelu[i])\n",
    "            dA,dW,db = self.affineBackward(relu_gradient,self.cacheAffine[i])\n",
    "            gradient_W[i] = dW\n",
    "            gradient_b[i] = db\n",
    "        \n",
    "        return (gradient_W,gradient_b)\n",
    "        # last layer \n",
    "            # affineBackward\n",
    "            # set dW[last layer], db[last layer]\n",
    "            \n",
    "        # for layers except last layer\n",
    "            # relu_Backward\n",
    "            # affineBackward\n",
    "            # set dW[layer], db[layer]\n",
    "    \n",
    "        # for part2: lambd > 0, you need to change the definition accordingly\n",
    "    \n",
    "    def updateParameters(self, gradients, learning_rate, lambd=0):\n",
    "        \"\"\"\n",
    "        Expected Functionality:\n",
    "        use gradients returned by backprop to update the parameters.\n",
    "        \"\"\"\n",
    "        gradient_W = gradients[0]\n",
    "        gradient_b = gradients[1]\n",
    "        \n",
    "        for i in range(1,self.num_layers-1):\n",
    "            self.W[i] -= learning_rate * gradient_W[i]\n",
    "            self.b[i] -= learning_rate * gradient_b[i]\n",
    "        \n",
    "        self.W[self.num_layers-1] -= learning_rate * gradient_W[self.num_layers-1]\n",
    "        \n",
    "    \n",
    "    def evaluate(self, X_test, Y_test):\n",
    "        '''\n",
    "        X: X_test (3472 dimensions, 10000 examples)\n",
    "        Y: Y_test (1 dimension, 10000 examples)\n",
    "        \n",
    "        Expected Functionality: \n",
    "        print accuracy on test set\n",
    "        '''\n",
    "        fdforward_output = self.feedforward(X_test)\n",
    "        prediction = np.argmax(fdforward_output, axis = 0)\n",
    "        count = 0\n",
    "        for i in range(10000):\n",
    "            if prediction[i] == Y_test[:,i][0]:\n",
    "                count += 1\n",
    "        return count/10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    \n",
    "    def unpickle(self,file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "    \n",
    "    def load_train_data(self):\n",
    "        '''\n",
    "        loads training data: 50,000 examples with 3072 features\n",
    "        '''\n",
    "        X_train = None\n",
    "        Y_train = None\n",
    "        for i in range(1, 6):\n",
    "            pickleFile = self.unpickle('cifar-10-batches-py/data_batch_{}'.format(i))\n",
    "            dataX = pickleFile[b'data']\n",
    "            dataY = pickleFile[b'labels']\n",
    "            if type(X_train) is np.ndarray:\n",
    "                X_train = np.concatenate((X_train, dataX))\n",
    "                Y_train = np.concatenate((Y_train, dataY))\n",
    "            else:\n",
    "                X_train = dataX\n",
    "                Y_train = dataY\n",
    "\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "\n",
    "        return X_train.T, Y_train.T\n",
    "\n",
    "    def load_test_data(self):\n",
    "        '''\n",
    "        loads testing data: 10,000 examples with 3072 features\n",
    "        '''\n",
    "        X_test = None\n",
    "        Y_test = None\n",
    "        pickleFile = self.unpickle('cifar-10-batches-py/test_batch')\n",
    "        dataX = pickleFile[b'data']\n",
    "        dataY = pickleFile[b'labels']\n",
    "        if type(X_test) is np.ndarray:\n",
    "            X_test = np.concatenate((X_test, dataX))\n",
    "            Y_test = np.concatenate((Y_test, dataY))\n",
    "        else:\n",
    "            X_test = np.array(dataX)\n",
    "            Y_test = np.array(dataY)\n",
    "\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "        return X_test.T, Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train: (3072, 50000) -> 50000 examples, 3072 features\n",
      "Y_Train: (1, 50000) -> 50000 examples, 1 features\n",
      "X_Test: (3072, 10000) -> 10000 examples, 3072 features\n",
      "Y_Test: (1, 10000) -> 10000 examples, 1 features\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train = Loader().load_train_data()\n",
    "X_test, Y_test = Loader().load_test_data()\n",
    "\n",
    "print(\"X_Train: {} -> {} examples, {} features\".format(X_train.shape, X_train.shape[1], X_train.shape[0]))\n",
    "print(\"Y_Train: {} -> {} examples, {} features\".format(Y_train.shape, Y_train.shape[1], Y_train.shape[0]))\n",
    "print(\"X_Test: {} -> {} examples, {} features\".format(X_test.shape, X_test.shape[1], X_test.shape[0]))\n",
    "print(\"Y_Test: {} -> {} examples, {} features\".format(Y_test.shape, Y_test.shape[1], Y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration 0 is 2.30321112485\n",
      "loss in iteration 1 is 2.30270268747\n",
      "loss in iteration 2 is 2.30297767108\n",
      "loss in iteration 3 is 2.3032225299\n",
      "loss in iteration 4 is 2.30214368437\n",
      "loss in iteration 5 is 2.30272352915\n",
      "loss in iteration 6 is 2.30268218829\n",
      "loss in iteration 7 is 2.30194281459\n",
      "loss in iteration 8 is 2.30219076714\n",
      "loss in iteration 9 is 2.30316042745\n",
      "loss in iteration 10 is 2.30343753527\n",
      "loss in iteration 11 is 2.30278060059\n",
      "loss in iteration 12 is 2.30170414439\n",
      "loss in iteration 13 is 2.3023740463\n",
      "loss in iteration 14 is 2.30341103951\n",
      "loss in iteration 15 is 2.30294289897\n",
      "loss in iteration 16 is 2.30236046541\n",
      "loss in iteration 17 is 2.30177102558\n",
      "loss in iteration 18 is 2.30272625349\n",
      "loss in iteration 19 is 2.30215804599\n",
      "loss in iteration 20 is 2.30220333247\n",
      "loss in iteration 21 is 2.30350825358\n",
      "loss in iteration 22 is 2.3031379359\n",
      "loss in iteration 23 is 2.30222076664\n",
      "loss in iteration 24 is 2.30323434716\n",
      "loss in iteration 25 is 2.30346525538\n",
      "loss in iteration 26 is 2.30315075022\n",
      "loss in iteration 27 is 2.30320697794\n",
      "loss in iteration 28 is 2.30359440997\n",
      "loss in iteration 29 is 2.30267756558\n",
      "loss in iteration 30 is 2.30185097105\n",
      "loss in iteration 31 is 2.30253340941\n",
      "loss in iteration 32 is 2.30241857525\n",
      "loss in iteration 33 is 2.3021018829\n",
      "loss in iteration 34 is 2.30308584503\n",
      "loss in iteration 35 is 2.30295238478\n",
      "loss in iteration 36 is 2.30201617074\n",
      "loss in iteration 37 is 2.30206349565\n",
      "loss in iteration 38 is 2.30173561784\n",
      "loss in iteration 39 is 2.30351170145\n",
      "loss in iteration 40 is 2.30248809782\n",
      "loss in iteration 41 is 2.3027651838\n",
      "loss in iteration 42 is 2.30368708483\n",
      "loss in iteration 43 is 2.3027942435\n",
      "loss in iteration 44 is 2.30177566328\n",
      "loss in iteration 45 is 2.30201600979\n",
      "loss in iteration 46 is 2.30228627439\n",
      "loss in iteration 47 is 2.3009562572\n",
      "loss in iteration 48 is 2.30260392675\n",
      "loss in iteration 49 is 2.30344584361\n",
      "loss in iteration 50 is 2.30207069298\n",
      "loss in iteration 51 is 2.30244218072\n",
      "loss in iteration 52 is 2.30244553509\n",
      "loss in iteration 53 is 2.30194810614\n",
      "loss in iteration 54 is 2.30267871878\n",
      "loss in iteration 55 is 2.30240014698\n",
      "loss in iteration 56 is 2.30291139458\n",
      "loss in iteration 57 is 2.30262359806\n",
      "loss in iteration 58 is 2.30241201446\n",
      "loss in iteration 59 is 2.30331432026\n",
      "loss in iteration 60 is 2.30245116299\n",
      "loss in iteration 61 is 2.30292858992\n",
      "loss in iteration 62 is 2.30208919736\n",
      "loss in iteration 63 is 2.30337685662\n",
      "loss in iteration 64 is 2.30220544035\n",
      "loss in iteration 65 is 2.30248686394\n",
      "loss in iteration 66 is 2.30312083348\n",
      "loss in iteration 67 is 2.30276296417\n",
      "loss in iteration 68 is 2.30281614702\n",
      "loss in iteration 69 is 2.30223797429\n",
      "loss in iteration 70 is 2.30273950433\n",
      "loss in iteration 71 is 2.30208216354\n",
      "loss in iteration 72 is 2.3022553944\n",
      "loss in iteration 73 is 2.30190678603\n",
      "loss in iteration 74 is 2.30277823295\n",
      "loss in iteration 75 is 2.30328995909\n",
      "loss in iteration 76 is 2.30369023485\n",
      "loss in iteration 77 is 2.30222258092\n",
      "loss in iteration 78 is 2.30252923378\n",
      "loss in iteration 79 is 2.3027626483\n",
      "loss in iteration 80 is 2.30224788329\n",
      "loss in iteration 81 is 2.3029343911\n",
      "loss in iteration 82 is 2.30256607803\n",
      "loss in iteration 83 is 2.30372616721\n",
      "loss in iteration 84 is 2.30327388885\n",
      "loss in iteration 85 is 2.30330861369\n",
      "loss in iteration 86 is 2.30236161061\n",
      "loss in iteration 87 is 2.30281422225\n",
      "loss in iteration 88 is 2.30333737714\n",
      "loss in iteration 89 is 2.30241530173\n",
      "loss in iteration 90 is 2.30347337341\n",
      "loss in iteration 91 is 2.3024654687\n",
      "loss in iteration 92 is 2.30289382995\n",
      "loss in iteration 93 is 2.30254490765\n",
      "loss in iteration 94 is 2.3019685325\n",
      "loss in iteration 95 is 2.30243784268\n",
      "loss in iteration 96 is 2.30269529389\n",
      "loss in iteration 97 is 2.30269575654\n",
      "loss in iteration 98 is 2.30258028174\n",
      "loss in iteration 99 is 2.30321849754\n",
      "loss in iteration 100 is 2.3021164343\n",
      "loss in iteration 101 is 2.30249466575\n",
      "loss in iteration 102 is 2.30299812175\n",
      "loss in iteration 103 is 2.30238986823\n",
      "loss in iteration 104 is 2.30360038258\n",
      "loss in iteration 105 is 2.30194528113\n",
      "loss in iteration 106 is 2.30257670036\n",
      "loss in iteration 107 is 2.3035743896\n",
      "loss in iteration 108 is 2.3032775105\n",
      "loss in iteration 109 is 2.30290006198\n",
      "loss in iteration 110 is 2.30241521536\n",
      "loss in iteration 111 is 2.30235420321\n",
      "loss in iteration 112 is 2.30263734492\n",
      "loss in iteration 113 is 2.30273223066\n",
      "loss in iteration 114 is 2.30195357439\n",
      "loss in iteration 115 is 2.30215625698\n",
      "loss in iteration 116 is 2.30231801174\n",
      "loss in iteration 117 is 2.30264713018\n",
      "loss in iteration 118 is 2.30217950229\n",
      "loss in iteration 119 is 2.30328480917\n",
      "loss in iteration 120 is 2.30301120505\n",
      "loss in iteration 121 is 2.3023434223\n",
      "loss in iteration 122 is 2.30267349631\n",
      "loss in iteration 123 is 2.30212017\n",
      "loss in iteration 124 is 2.30369308017\n",
      "loss in iteration 125 is 2.30274869025\n",
      "loss in iteration 126 is 2.30222777846\n",
      "loss in iteration 127 is 2.30200278862\n",
      "loss in iteration 128 is 2.30213795869\n",
      "loss in iteration 129 is 2.30298324346\n",
      "loss in iteration 130 is 2.30219901162\n",
      "loss in iteration 131 is 2.302284813\n",
      "loss in iteration 132 is 2.3032024118\n",
      "loss in iteration 133 is 2.30204528345\n",
      "loss in iteration 134 is 2.30252125756\n",
      "loss in iteration 135 is 2.30207722618\n",
      "loss in iteration 136 is 2.30238552885\n",
      "loss in iteration 137 is 2.30292825904\n",
      "loss in iteration 138 is 2.30223671686\n",
      "loss in iteration 139 is 2.30301135436\n",
      "loss in iteration 140 is 2.30260819013\n",
      "loss in iteration 141 is 2.30251837494\n",
      "loss in iteration 142 is 2.30275033972\n",
      "loss in iteration 143 is 2.30344926761\n",
      "loss in iteration 144 is 2.30282900096\n",
      "loss in iteration 145 is 2.30271115346\n",
      "loss in iteration 146 is 2.30299047684\n",
      "loss in iteration 147 is 2.30326540184\n",
      "loss in iteration 148 is 2.30210514486\n",
      "loss in iteration 149 is 2.30195563525\n",
      "loss in iteration 150 is 2.30289744266\n",
      "loss in iteration 151 is 2.30331988834\n",
      "loss in iteration 152 is 2.30184043608\n",
      "loss in iteration 153 is 2.30271113342\n",
      "loss in iteration 154 is 2.30222173568\n",
      "loss in iteration 155 is 2.30170081643\n",
      "loss in iteration 156 is 2.30283232861\n",
      "loss in iteration 157 is 2.30227955184\n",
      "loss in iteration 158 is 2.30267755051\n",
      "loss in iteration 159 is 2.30260130926\n",
      "loss in iteration 160 is 2.30140707226\n",
      "loss in iteration 161 is 2.30326508826\n",
      "loss in iteration 162 is 2.30273072853\n",
      "loss in iteration 163 is 2.30226161703\n",
      "loss in iteration 164 is 2.30339393736\n",
      "loss in iteration 165 is 2.3029895011\n",
      "loss in iteration 166 is 2.30380412612\n",
      "loss in iteration 167 is 2.30229616582\n",
      "loss in iteration 168 is 2.30231654507\n",
      "loss in iteration 169 is 2.30229306847\n",
      "loss in iteration 170 is 2.30228189882\n",
      "loss in iteration 171 is 2.30288845764\n",
      "loss in iteration 172 is 2.30252556232\n",
      "loss in iteration 173 is 2.30318266961\n",
      "loss in iteration 174 is 2.30232667666\n",
      "loss in iteration 175 is 2.30282995896\n",
      "loss in iteration 176 is 2.30347349429\n",
      "loss in iteration 177 is 2.30330594422\n",
      "loss in iteration 178 is 2.30223701957\n",
      "loss in iteration 179 is 2.30269592122\n",
      "loss in iteration 180 is 2.30222223617\n",
      "loss in iteration 181 is 2.30225011525\n",
      "loss in iteration 182 is 2.30276904075\n",
      "loss in iteration 183 is 2.30358785962\n",
      "loss in iteration 184 is 2.30220744624\n",
      "loss in iteration 185 is 2.30279811181\n",
      "loss in iteration 186 is 2.30204470257\n",
      "loss in iteration 187 is 2.30277258372\n",
      "loss in iteration 188 is 2.30225267589\n",
      "loss in iteration 189 is 2.30275983006\n",
      "loss in iteration 190 is 2.30323174915\n",
      "loss in iteration 191 is 2.30276319472\n",
      "loss in iteration 192 is 2.30106472802\n",
      "loss in iteration 193 is 2.30296584033\n",
      "loss in iteration 194 is 2.30323981663\n",
      "loss in iteration 195 is 2.30414372821\n",
      "loss in iteration 196 is 2.30243459377\n",
      "loss in iteration 197 is 2.30225050276\n",
      "loss in iteration 198 is 2.3028736952\n",
      "loss in iteration 199 is 2.30171057856\n",
      "loss in iteration 200 is 2.30230056297\n",
      "loss in iteration 201 is 2.30285419938\n",
      "loss in iteration 202 is 2.30276503906\n",
      "loss in iteration 203 is 2.30253100613\n",
      "loss in iteration 204 is 2.30298502598\n",
      "loss in iteration 205 is 2.30299863809\n",
      "loss in iteration 206 is 2.30277800515\n",
      "loss in iteration 207 is 2.30276042621\n",
      "loss in iteration 208 is 2.3034297546\n",
      "loss in iteration 209 is 2.30150326311\n",
      "loss in iteration 210 is 2.30294379584\n",
      "loss in iteration 211 is 2.30209661874\n",
      "loss in iteration 212 is 2.3021813109\n",
      "loss in iteration 213 is 2.30271181118\n",
      "loss in iteration 214 is 2.30331146321\n",
      "loss in iteration 215 is 2.30373984819\n",
      "loss in iteration 216 is 2.30236433421\n",
      "loss in iteration 217 is 2.30229045789\n",
      "loss in iteration 218 is 2.30205872477\n",
      "loss in iteration 219 is 2.30354398466\n",
      "loss in iteration 220 is 2.30262185281\n",
      "loss in iteration 221 is 2.30203634836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration 222 is 2.30286849151\n",
      "loss in iteration 223 is 2.30221540297\n",
      "loss in iteration 224 is 2.30192161755\n",
      "loss in iteration 225 is 2.30232432733\n",
      "loss in iteration 226 is 2.30193610104\n",
      "loss in iteration 227 is 2.30307099772\n",
      "loss in iteration 228 is 2.30281875184\n",
      "loss in iteration 229 is 2.30188244264\n",
      "loss in iteration 230 is 2.30269753767\n",
      "loss in iteration 231 is 2.30196874795\n",
      "loss in iteration 232 is 2.30318180885\n",
      "loss in iteration 233 is 2.30254552597\n",
      "loss in iteration 234 is 2.30257776261\n",
      "loss in iteration 235 is 2.3021220625\n",
      "loss in iteration 236 is 2.30327381355\n",
      "loss in iteration 237 is 2.30283858614\n",
      "loss in iteration 238 is 2.3025058973\n",
      "loss in iteration 239 is 2.30276189269\n",
      "loss in iteration 240 is 2.30251663847\n",
      "loss in iteration 241 is 2.3027623794\n",
      "loss in iteration 242 is 2.30241217452\n",
      "loss in iteration 243 is 2.30253876927\n",
      "loss in iteration 244 is 2.30228520097\n",
      "loss in iteration 245 is 2.30331161001\n",
      "loss in iteration 246 is 2.3022414723\n",
      "loss in iteration 247 is 2.30211176367\n",
      "loss in iteration 248 is 2.30257609225\n",
      "loss in iteration 249 is 2.30281078143\n",
      "loss in iteration 250 is 2.30282889229\n",
      "loss in iteration 251 is 2.30212646248\n",
      "loss in iteration 252 is 2.30275286381\n",
      "loss in iteration 253 is 2.30348345271\n",
      "loss in iteration 254 is 2.30270608564\n",
      "loss in iteration 255 is 2.30243824797\n",
      "loss in iteration 256 is 2.30207105265\n",
      "loss in iteration 257 is 2.3029834579\n",
      "loss in iteration 258 is 2.30260635239\n",
      "loss in iteration 259 is 2.30300654323\n",
      "loss in iteration 260 is 2.30274434831\n",
      "loss in iteration 261 is 2.30290337641\n",
      "loss in iteration 262 is 2.30330506893\n",
      "loss in iteration 263 is 2.30265676948\n",
      "loss in iteration 264 is 2.3031054952\n",
      "loss in iteration 265 is 2.30164281761\n",
      "loss in iteration 266 is 2.30300879171\n",
      "loss in iteration 267 is 2.30188718495\n",
      "loss in iteration 268 is 2.30266404983\n",
      "loss in iteration 269 is 2.30271692452\n",
      "loss in iteration 270 is 2.30149797577\n",
      "loss in iteration 271 is 2.30094812644\n",
      "loss in iteration 272 is 2.30245331028\n",
      "loss in iteration 273 is 2.30285449516\n",
      "loss in iteration 274 is 2.30262210427\n",
      "loss in iteration 275 is 2.30218873628\n",
      "loss in iteration 276 is 2.3019494484\n",
      "loss in iteration 277 is 2.30141209333\n",
      "loss in iteration 278 is 2.30284716824\n",
      "loss in iteration 279 is 2.30283910257\n",
      "loss in iteration 280 is 2.30251719069\n",
      "loss in iteration 281 is 2.30299675124\n",
      "loss in iteration 282 is 2.30224731192\n",
      "loss in iteration 283 is 2.30301581449\n",
      "loss in iteration 284 is 2.30199844534\n",
      "loss in iteration 285 is 2.30263098689\n",
      "loss in iteration 286 is 2.30261312445\n",
      "loss in iteration 287 is 2.30301352234\n",
      "loss in iteration 288 is 2.30174604258\n",
      "loss in iteration 289 is 2.30329421734\n",
      "loss in iteration 290 is 2.30311339049\n",
      "loss in iteration 291 is 2.30219299272\n",
      "loss in iteration 292 is 2.30192473337\n",
      "loss in iteration 293 is 2.30279583888\n",
      "loss in iteration 294 is 2.30300584821\n",
      "loss in iteration 295 is 2.30213813806\n",
      "loss in iteration 296 is 2.30234497763\n",
      "loss in iteration 297 is 2.30220588745\n",
      "loss in iteration 298 is 2.30331135972\n",
      "loss in iteration 299 is 2.30255294716\n",
      "loss in iteration 300 is 2.30143658095\n",
      "loss in iteration 301 is 2.30276041179\n",
      "loss in iteration 302 is 2.30195289488\n",
      "loss in iteration 303 is 2.30279116761\n",
      "loss in iteration 304 is 2.30279670892\n",
      "loss in iteration 305 is 2.30206469561\n",
      "loss in iteration 306 is 2.30236061011\n",
      "loss in iteration 307 is 2.30129196256\n",
      "loss in iteration 308 is 2.30360635567\n",
      "loss in iteration 309 is 2.30200676017\n",
      "loss in iteration 310 is 2.30396107906\n",
      "loss in iteration 311 is 2.30295743303\n",
      "loss in iteration 312 is 2.30240863846\n",
      "loss in iteration 313 is 2.30069941225\n",
      "loss in iteration 314 is 2.30200997859\n",
      "loss in iteration 315 is 2.30230060687\n",
      "loss in iteration 316 is 2.30307278258\n",
      "loss in iteration 317 is 2.30204316864\n",
      "loss in iteration 318 is 2.30328266052\n",
      "loss in iteration 319 is 2.30315121134\n",
      "loss in iteration 320 is 2.30264885232\n",
      "loss in iteration 321 is 2.30212190658\n",
      "loss in iteration 322 is 2.30290855371\n",
      "loss in iteration 323 is 2.30360395405\n",
      "loss in iteration 324 is 2.30332137144\n",
      "loss in iteration 325 is 2.302356075\n",
      "loss in iteration 326 is 2.30192104288\n",
      "loss in iteration 327 is 2.30233869206\n",
      "loss in iteration 328 is 2.3036764643\n",
      "loss in iteration 329 is 2.30335251824\n",
      "loss in iteration 330 is 2.3036463645\n",
      "loss in iteration 331 is 2.30183603078\n",
      "loss in iteration 332 is 2.30164424264\n",
      "loss in iteration 333 is 2.30287189985\n",
      "loss in iteration 334 is 2.30263585432\n",
      "loss in iteration 335 is 2.3028441234\n",
      "loss in iteration 336 is 2.30312848742\n",
      "loss in iteration 337 is 2.30229362565\n",
      "loss in iteration 338 is 2.30209647398\n",
      "loss in iteration 339 is 2.30242121765\n",
      "loss in iteration 340 is 2.30284980757\n",
      "loss in iteration 341 is 2.3025821913\n",
      "loss in iteration 342 is 2.30311647144\n",
      "loss in iteration 343 is 2.30354677078\n",
      "loss in iteration 344 is 2.30318975105\n",
      "loss in iteration 345 is 2.30294292196\n",
      "loss in iteration 346 is 2.30302864734\n",
      "loss in iteration 347 is 2.30154986163\n",
      "loss in iteration 348 is 2.30325443764\n",
      "loss in iteration 349 is 2.30282368271\n",
      "loss in iteration 350 is 2.30294904746\n",
      "loss in iteration 351 is 2.30107711273\n",
      "loss in iteration 352 is 2.30340569791\n",
      "loss in iteration 353 is 2.30312509264\n",
      "loss in iteration 354 is 2.30264717508\n",
      "loss in iteration 355 is 2.30230909849\n",
      "loss in iteration 356 is 2.30288162318\n",
      "loss in iteration 357 is 2.3026062573\n",
      "loss in iteration 358 is 2.30274282616\n",
      "loss in iteration 359 is 2.30241092504\n",
      "loss in iteration 360 is 2.3029382477\n",
      "loss in iteration 361 is 2.30294299911\n",
      "loss in iteration 362 is 2.30180973708\n",
      "loss in iteration 363 is 2.30206912299\n",
      "loss in iteration 364 is 2.30238262418\n",
      "loss in iteration 365 is 2.30284898926\n",
      "loss in iteration 366 is 2.30186693347\n",
      "loss in iteration 367 is 2.30237886743\n",
      "loss in iteration 368 is 2.30304851128\n",
      "loss in iteration 369 is 2.30287201099\n",
      "loss in iteration 370 is 2.30304723198\n",
      "loss in iteration 371 is 2.30251493283\n",
      "loss in iteration 372 is 2.30328731009\n",
      "loss in iteration 373 is 2.30254522289\n",
      "loss in iteration 374 is 2.30362708594\n",
      "loss in iteration 375 is 2.3030270117\n",
      "loss in iteration 376 is 2.3017880965\n",
      "loss in iteration 377 is 2.30328638848\n",
      "loss in iteration 378 is 2.30260078316\n",
      "loss in iteration 379 is 2.30212236479\n",
      "loss in iteration 380 is 2.30197777419\n",
      "loss in iteration 381 is 2.30256428611\n",
      "loss in iteration 382 is 2.30320498176\n",
      "loss in iteration 383 is 2.30223268175\n",
      "loss in iteration 384 is 2.30268119139\n",
      "loss in iteration 385 is 2.30245130657\n",
      "loss in iteration 386 is 2.30191299618\n",
      "loss in iteration 387 is 2.30327135074\n",
      "loss in iteration 388 is 2.30330555488\n",
      "loss in iteration 389 is 2.30242783511\n",
      "loss in iteration 390 is 2.30323514547\n",
      "loss in iteration 391 is 2.30135467872\n",
      "loss in iteration 392 is 2.30340978444\n",
      "loss in iteration 393 is 2.30232382535\n",
      "loss in iteration 394 is 2.30300805551\n",
      "loss in iteration 395 is 2.30332539981\n",
      "loss in iteration 396 is 2.30184532283\n",
      "loss in iteration 397 is 2.301992288\n",
      "loss in iteration 398 is 2.30275274225\n",
      "loss in iteration 399 is 2.30325329216\n",
      "loss in iteration 400 is 2.30198715958\n",
      "loss in iteration 401 is 2.30223529634\n",
      "loss in iteration 402 is 2.30354934926\n",
      "loss in iteration 403 is 2.30270337255\n",
      "loss in iteration 404 is 2.30248126201\n",
      "loss in iteration 405 is 2.30181709255\n",
      "loss in iteration 406 is 2.30213836512\n",
      "loss in iteration 407 is 2.30251237427\n",
      "loss in iteration 408 is 2.30222406025\n",
      "loss in iteration 409 is 2.30184298538\n",
      "loss in iteration 410 is 2.30177417994\n",
      "loss in iteration 411 is 2.30141581324\n",
      "loss in iteration 412 is 2.30246152527\n",
      "loss in iteration 413 is 2.3030209367\n",
      "loss in iteration 414 is 2.30236407643\n",
      "loss in iteration 415 is 2.30273146938\n",
      "loss in iteration 416 is 2.30293167757\n",
      "loss in iteration 417 is 2.30326279012\n",
      "loss in iteration 418 is 2.30304716333\n",
      "loss in iteration 419 is 2.30327667621\n",
      "loss in iteration 420 is 2.30281083697\n",
      "loss in iteration 421 is 2.30327928451\n",
      "loss in iteration 422 is 2.30238007663\n",
      "loss in iteration 423 is 2.30328614793\n",
      "loss in iteration 424 is 2.30342975879\n",
      "loss in iteration 425 is 2.30257326747\n",
      "loss in iteration 426 is 2.30324320923\n",
      "loss in iteration 427 is 2.30291158655\n",
      "loss in iteration 428 is 2.30162120725\n",
      "loss in iteration 429 is 2.3024154753\n",
      "loss in iteration 430 is 2.30216633704\n",
      "loss in iteration 431 is 2.30239363744\n",
      "loss in iteration 432 is 2.30299420797\n",
      "loss in iteration 433 is 2.30288115696\n",
      "loss in iteration 434 is 2.30337952937\n",
      "loss in iteration 435 is 2.30370101223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration 436 is 2.30295118665\n",
      "loss in iteration 437 is 2.30134983097\n",
      "loss in iteration 438 is 2.30233278316\n",
      "loss in iteration 439 is 2.30139466374\n",
      "loss in iteration 440 is 2.30209302331\n",
      "loss in iteration 441 is 2.30306592927\n",
      "loss in iteration 442 is 2.30146313424\n",
      "loss in iteration 443 is 2.30215255777\n",
      "loss in iteration 444 is 2.3032733605\n",
      "loss in iteration 445 is 2.30203903198\n",
      "loss in iteration 446 is 2.30408926477\n",
      "loss in iteration 447 is 2.30331678131\n",
      "loss in iteration 448 is 2.30381154615\n",
      "loss in iteration 449 is 2.30228311903\n",
      "loss in iteration 450 is 2.30273177228\n",
      "loss in iteration 451 is 2.30175850081\n",
      "loss in iteration 452 is 2.30291618299\n",
      "loss in iteration 453 is 2.30210369456\n",
      "loss in iteration 454 is 2.3032694125\n",
      "loss in iteration 455 is 2.30236848753\n",
      "loss in iteration 456 is 2.30314088753\n",
      "loss in iteration 457 is 2.30398272059\n",
      "loss in iteration 458 is 2.3029109284\n",
      "loss in iteration 459 is 2.30282095822\n",
      "loss in iteration 460 is 2.30252523545\n",
      "loss in iteration 461 is 2.30227750998\n",
      "loss in iteration 462 is 2.30196504252\n",
      "loss in iteration 463 is 2.30341176511\n",
      "loss in iteration 464 is 2.303206839\n",
      "loss in iteration 465 is 2.30252263789\n",
      "loss in iteration 466 is 2.3027483934\n",
      "loss in iteration 467 is 2.30269065923\n",
      "loss in iteration 468 is 2.30275397569\n",
      "loss in iteration 469 is 2.30263191504\n",
      "loss in iteration 470 is 2.30212622369\n",
      "loss in iteration 471 is 2.30260331143\n",
      "loss in iteration 472 is 2.30263491642\n",
      "loss in iteration 473 is 2.30333876793\n",
      "loss in iteration 474 is 2.30303913583\n",
      "loss in iteration 475 is 2.30192724428\n",
      "loss in iteration 476 is 2.3033241239\n",
      "loss in iteration 477 is 2.30314100837\n",
      "loss in iteration 478 is 2.30222768999\n",
      "loss in iteration 479 is 2.30366131957\n",
      "loss in iteration 480 is 2.30199220787\n",
      "loss in iteration 481 is 2.30216062062\n",
      "loss in iteration 482 is 2.3036236938\n",
      "loss in iteration 483 is 2.30306748797\n",
      "loss in iteration 484 is 2.30342735099\n",
      "loss in iteration 485 is 2.30239271289\n",
      "loss in iteration 486 is 2.30290488194\n",
      "loss in iteration 487 is 2.30292587532\n",
      "loss in iteration 488 is 2.30221748641\n",
      "loss in iteration 489 is 2.30313803892\n",
      "loss in iteration 490 is 2.302413885\n",
      "loss in iteration 491 is 2.30297749822\n",
      "loss in iteration 492 is 2.30359085794\n",
      "loss in iteration 493 is 2.30180607288\n",
      "loss in iteration 494 is 2.30245844834\n",
      "loss in iteration 495 is 2.30235644609\n",
      "loss in iteration 496 is 2.30235130467\n",
      "loss in iteration 497 is 2.3025495513\n",
      "loss in iteration 498 is 2.30323075186\n",
      "loss in iteration 499 is 2.30314110689\n",
      "loss in iteration 500 is 2.3018127214\n",
      "loss in iteration 501 is 2.30280024966\n",
      "loss in iteration 502 is 2.30284570853\n",
      "loss in iteration 503 is 2.30352618142\n",
      "loss in iteration 504 is 2.30226808343\n",
      "loss in iteration 505 is 2.3016208928\n",
      "loss in iteration 506 is 2.30310196538\n",
      "loss in iteration 507 is 2.30306313836\n",
      "loss in iteration 508 is 2.30222450312\n",
      "loss in iteration 509 is 2.30248345166\n",
      "loss in iteration 510 is 2.30274869769\n",
      "loss in iteration 511 is 2.30294687036\n",
      "loss in iteration 512 is 2.30281079316\n",
      "loss in iteration 513 is 2.30294873867\n",
      "loss in iteration 514 is 2.30246088438\n",
      "loss in iteration 515 is 2.30303510584\n",
      "loss in iteration 516 is 2.30231479294\n",
      "loss in iteration 517 is 2.30285442559\n",
      "loss in iteration 518 is 2.30267948152\n",
      "loss in iteration 519 is 2.30323890973\n",
      "loss in iteration 520 is 2.30203400954\n",
      "loss in iteration 521 is 2.30286240047\n",
      "loss in iteration 522 is 2.30191654117\n",
      "loss in iteration 523 is 2.30296421869\n",
      "loss in iteration 524 is 2.30318453631\n",
      "loss in iteration 525 is 2.30289152119\n",
      "loss in iteration 526 is 2.30383715542\n",
      "loss in iteration 527 is 2.30274880033\n",
      "loss in iteration 528 is 2.30314166749\n",
      "loss in iteration 529 is 2.30244897041\n",
      "loss in iteration 530 is 2.30160344315\n",
      "loss in iteration 531 is 2.30223387312\n",
      "loss in iteration 532 is 2.3024139258\n",
      "loss in iteration 533 is 2.30298962551\n",
      "loss in iteration 534 is 2.30178563621\n",
      "loss in iteration 535 is 2.30272686649\n",
      "loss in iteration 536 is 2.30302214991\n",
      "loss in iteration 537 is 2.30240180047\n",
      "loss in iteration 538 is 2.30314554611\n",
      "loss in iteration 539 is 2.30270411544\n",
      "loss in iteration 540 is 2.30245136496\n",
      "loss in iteration 541 is 2.30261391284\n",
      "loss in iteration 542 is 2.3032915294\n",
      "loss in iteration 543 is 2.30226670303\n",
      "loss in iteration 544 is 2.30215873439\n",
      "loss in iteration 545 is 2.30284497455\n",
      "loss in iteration 546 is 2.30145973926\n",
      "loss in iteration 547 is 2.30298916203\n",
      "loss in iteration 548 is 2.30306701841\n",
      "loss in iteration 549 is 2.30345498364\n",
      "loss in iteration 550 is 2.30209510492\n",
      "loss in iteration 551 is 2.30274166619\n",
      "loss in iteration 552 is 2.30334309705\n",
      "loss in iteration 553 is 2.30098237227\n",
      "loss in iteration 554 is 2.3028721386\n",
      "loss in iteration 555 is 2.30352262994\n",
      "loss in iteration 556 is 2.30302594079\n",
      "loss in iteration 557 is 2.30283929853\n",
      "loss in iteration 558 is 2.30196731105\n",
      "loss in iteration 559 is 2.30280155341\n",
      "loss in iteration 560 is 2.30305988949\n",
      "loss in iteration 561 is 2.30270236587\n",
      "loss in iteration 562 is 2.30259095455\n",
      "loss in iteration 563 is 2.30266427014\n",
      "loss in iteration 564 is 2.3027656074\n",
      "loss in iteration 565 is 2.30385336151\n",
      "loss in iteration 566 is 2.3019551089\n",
      "loss in iteration 567 is 2.30127231466\n",
      "loss in iteration 568 is 2.303028733\n",
      "loss in iteration 569 is 2.30305138536\n",
      "loss in iteration 570 is 2.30357734542\n",
      "loss in iteration 571 is 2.30393663742\n",
      "loss in iteration 572 is 2.30241665288\n",
      "loss in iteration 573 is 2.30275702575\n",
      "loss in iteration 574 is 2.30289248775\n",
      "loss in iteration 575 is 2.30268981749\n",
      "loss in iteration 576 is 2.30245874592\n",
      "loss in iteration 577 is 2.30182600339\n",
      "loss in iteration 578 is 2.3023682592\n",
      "loss in iteration 579 is 2.30291450416\n",
      "loss in iteration 580 is 2.30242437114\n",
      "loss in iteration 581 is 2.30239956195\n",
      "loss in iteration 582 is 2.30289709438\n",
      "loss in iteration 583 is 2.30291077384\n",
      "loss in iteration 584 is 2.30308549453\n",
      "loss in iteration 585 is 2.30258312526\n",
      "loss in iteration 586 is 2.30335021583\n",
      "loss in iteration 587 is 2.30285487782\n",
      "loss in iteration 588 is 2.30372754523\n",
      "loss in iteration 589 is 2.30292181822\n",
      "loss in iteration 590 is 2.30169648996\n",
      "loss in iteration 591 is 2.30273680632\n",
      "loss in iteration 592 is 2.30327120226\n",
      "loss in iteration 593 is 2.30311317097\n",
      "loss in iteration 594 is 2.3016069418\n",
      "loss in iteration 595 is 2.30198726297\n",
      "loss in iteration 596 is 2.3020060527\n",
      "loss in iteration 597 is 2.30227275844\n",
      "loss in iteration 598 is 2.30150857814\n",
      "loss in iteration 599 is 2.30213643373\n",
      "loss in iteration 600 is 2.30259001437\n",
      "loss in iteration 601 is 2.30241770242\n",
      "loss in iteration 602 is 2.30282249904\n",
      "loss in iteration 603 is 2.30285364789\n",
      "loss in iteration 604 is 2.30172176144\n",
      "loss in iteration 605 is 2.30322493639\n",
      "loss in iteration 606 is 2.30277349367\n",
      "loss in iteration 607 is 2.30234543642\n",
      "loss in iteration 608 is 2.30252983725\n",
      "loss in iteration 609 is 2.3024723847\n",
      "loss in iteration 610 is 2.30315917321\n",
      "loss in iteration 611 is 2.30268217357\n",
      "loss in iteration 612 is 2.30232675114\n",
      "loss in iteration 613 is 2.30248181024\n",
      "loss in iteration 614 is 2.30191977804\n",
      "loss in iteration 615 is 2.30232995121\n",
      "loss in iteration 616 is 2.30218021534\n",
      "loss in iteration 617 is 2.30300652288\n",
      "loss in iteration 618 is 2.30216108856\n",
      "loss in iteration 619 is 2.30259543698\n",
      "loss in iteration 620 is 2.30339715246\n",
      "loss in iteration 621 is 2.30272595282\n",
      "loss in iteration 622 is 2.30257943056\n",
      "loss in iteration 623 is 2.3031609867\n",
      "loss in iteration 624 is 2.30293454243\n",
      "loss in iteration 625 is 2.30239982936\n",
      "loss in iteration 626 is 2.302539159\n",
      "loss in iteration 627 is 2.30252824367\n",
      "loss in iteration 628 is 2.30247160453\n",
      "loss in iteration 629 is 2.30147753926\n",
      "loss in iteration 630 is 2.30213959228\n",
      "loss in iteration 631 is 2.30345962806\n",
      "loss in iteration 632 is 2.30247460673\n",
      "loss in iteration 633 is 2.30227447953\n",
      "loss in iteration 634 is 2.30218152298\n",
      "loss in iteration 635 is 2.30282432558\n",
      "loss in iteration 636 is 2.30313055994\n",
      "loss in iteration 637 is 2.30203929371\n",
      "loss in iteration 638 is 2.30237348018\n",
      "loss in iteration 639 is 2.30333595644\n",
      "loss in iteration 640 is 2.30263101764\n",
      "loss in iteration 641 is 2.30177573286\n",
      "loss in iteration 642 is 2.30247430201\n",
      "loss in iteration 643 is 2.30267995372\n",
      "loss in iteration 644 is 2.30297100158\n",
      "loss in iteration 645 is 2.30260117452\n",
      "loss in iteration 646 is 2.30277707899\n",
      "loss in iteration 647 is 2.30221052484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration 648 is 2.30326729293\n",
      "loss in iteration 649 is 2.30197721018\n",
      "loss in iteration 650 is 2.30315796034\n",
      "loss in iteration 651 is 2.30319743766\n",
      "loss in iteration 652 is 2.302136316\n",
      "loss in iteration 653 is 2.30322678263\n",
      "loss in iteration 654 is 2.30319333696\n",
      "loss in iteration 655 is 2.30369589868\n",
      "loss in iteration 656 is 2.30335114029\n",
      "loss in iteration 657 is 2.30308436945\n",
      "loss in iteration 658 is 2.30188345022\n",
      "loss in iteration 659 is 2.30238192665\n",
      "loss in iteration 660 is 2.30266734058\n",
      "loss in iteration 661 is 2.30235989637\n",
      "loss in iteration 662 is 2.30251099397\n",
      "loss in iteration 663 is 2.30203676728\n",
      "loss in iteration 664 is 2.30253218507\n",
      "loss in iteration 665 is 2.30246616331\n",
      "loss in iteration 666 is 2.30312952598\n",
      "loss in iteration 667 is 2.30239171024\n",
      "loss in iteration 668 is 2.30233079979\n",
      "loss in iteration 669 is 2.30245841894\n",
      "loss in iteration 670 is 2.30267197374\n",
      "loss in iteration 671 is 2.30216359002\n",
      "loss in iteration 672 is 2.30231468917\n",
      "loss in iteration 673 is 2.30305691499\n",
      "loss in iteration 674 is 2.3020254668\n",
      "loss in iteration 675 is 2.30204243301\n",
      "loss in iteration 676 is 2.30279766022\n",
      "loss in iteration 677 is 2.30312022217\n",
      "loss in iteration 678 is 2.30228814438\n",
      "loss in iteration 679 is 2.30322414975\n",
      "loss in iteration 680 is 2.30238090325\n",
      "loss in iteration 681 is 2.30291517107\n",
      "loss in iteration 682 is 2.30243096033\n",
      "loss in iteration 683 is 2.30274696121\n",
      "loss in iteration 684 is 2.30271533631\n",
      "loss in iteration 685 is 2.30309134631\n",
      "loss in iteration 686 is 2.30286683679\n",
      "loss in iteration 687 is 2.30184875614\n",
      "loss in iteration 688 is 2.30190046245\n",
      "loss in iteration 689 is 2.30338901979\n",
      "loss in iteration 690 is 2.30171135862\n",
      "loss in iteration 691 is 2.30267735782\n",
      "loss in iteration 692 is 2.30255377954\n",
      "loss in iteration 693 is 2.30233783071\n",
      "loss in iteration 694 is 2.30155693573\n",
      "loss in iteration 695 is 2.30285232614\n",
      "loss in iteration 696 is 2.30309022791\n",
      "loss in iteration 697 is 2.30147292346\n",
      "loss in iteration 698 is 2.30242771255\n",
      "loss in iteration 699 is 2.30255260293\n",
      "loss in iteration 700 is 2.30223013494\n",
      "loss in iteration 701 is 2.30279808254\n",
      "loss in iteration 702 is 2.30288039761\n",
      "loss in iteration 703 is 2.3028484913\n",
      "loss in iteration 704 is 2.3028674908\n",
      "loss in iteration 705 is 2.30286728062\n",
      "loss in iteration 706 is 2.30209195983\n",
      "loss in iteration 707 is 2.30185085582\n",
      "loss in iteration 708 is 2.3016649\n",
      "loss in iteration 709 is 2.303431746\n",
      "loss in iteration 710 is 2.30223256611\n",
      "loss in iteration 711 is 2.30294898749\n",
      "loss in iteration 712 is 2.30315677581\n",
      "loss in iteration 713 is 2.30242316924\n",
      "loss in iteration 714 is 2.30267913584\n",
      "loss in iteration 715 is 2.30394648654\n",
      "loss in iteration 716 is 2.30209217986\n",
      "loss in iteration 717 is 2.30260686388\n",
      "loss in iteration 718 is 2.3026243389\n",
      "loss in iteration 719 is 2.30315752912\n",
      "loss in iteration 720 is 2.30351176886\n",
      "loss in iteration 721 is 2.30198250011\n",
      "loss in iteration 722 is 2.30244302964\n",
      "loss in iteration 723 is 2.30265262221\n",
      "loss in iteration 724 is 2.30290686454\n",
      "loss in iteration 725 is 2.30241188685\n",
      "loss in iteration 726 is 2.30276433567\n",
      "loss in iteration 727 is 2.30241207518\n",
      "loss in iteration 728 is 2.30278029405\n",
      "loss in iteration 729 is 2.30163950604\n",
      "loss in iteration 730 is 2.30277376272\n",
      "loss in iteration 731 is 2.30308171678\n",
      "loss in iteration 732 is 2.30301329317\n",
      "loss in iteration 733 is 2.30224647956\n",
      "loss in iteration 734 is 2.30306892792\n",
      "loss in iteration 735 is 2.30251525112\n",
      "loss in iteration 736 is 2.30335070672\n",
      "loss in iteration 737 is 2.30195413083\n",
      "loss in iteration 738 is 2.30247010566\n",
      "loss in iteration 739 is 2.30339908732\n",
      "loss in iteration 740 is 2.30097874661\n",
      "loss in iteration 741 is 2.30362256276\n",
      "loss in iteration 742 is 2.30239289876\n",
      "loss in iteration 743 is 2.30212912152\n",
      "loss in iteration 744 is 2.30218916994\n",
      "loss in iteration 745 is 2.30322807074\n",
      "loss in iteration 746 is 2.30272314296\n",
      "loss in iteration 747 is 2.30383969119\n",
      "loss in iteration 748 is 2.30309097276\n",
      "loss in iteration 749 is 2.30351603585\n",
      "loss in iteration 750 is 2.30257484063\n",
      "loss in iteration 751 is 2.30333263482\n",
      "loss in iteration 752 is 2.3025139274\n",
      "loss in iteration 753 is 2.30236281012\n",
      "loss in iteration 754 is 2.30332154777\n",
      "loss in iteration 755 is 2.30254990486\n",
      "loss in iteration 756 is 2.30308562971\n",
      "loss in iteration 757 is 2.30221756067\n",
      "loss in iteration 758 is 2.30070761036\n",
      "loss in iteration 759 is 2.30189680461\n",
      "loss in iteration 760 is 2.30235267856\n",
      "loss in iteration 761 is 2.30171944477\n",
      "loss in iteration 762 is 2.30321359509\n",
      "loss in iteration 763 is 2.30185684492\n",
      "loss in iteration 764 is 2.30253375463\n",
      "loss in iteration 765 is 2.30283613177\n",
      "loss in iteration 766 is 2.30264734792\n",
      "loss in iteration 767 is 2.3026618875\n",
      "loss in iteration 768 is 2.30320109949\n",
      "loss in iteration 769 is 2.30321568256\n",
      "loss in iteration 770 is 2.30329197983\n",
      "loss in iteration 771 is 2.30287883691\n",
      "loss in iteration 772 is 2.30183749931\n",
      "loss in iteration 773 is 2.30290690148\n",
      "loss in iteration 774 is 2.30283826534\n",
      "loss in iteration 775 is 2.30282758613\n",
      "loss in iteration 776 is 2.30277621558\n",
      "loss in iteration 777 is 2.30302485561\n",
      "loss in iteration 778 is 2.30189719042\n",
      "loss in iteration 779 is 2.30317041179\n",
      "loss in iteration 780 is 2.30297832585\n",
      "loss in iteration 781 is 2.30261338097\n",
      "loss in iteration 782 is 2.30257227664\n",
      "loss in iteration 783 is 2.30255477419\n",
      "loss in iteration 784 is 2.30282995233\n",
      "loss in iteration 785 is 2.30314039906\n",
      "loss in iteration 786 is 2.30338550681\n",
      "loss in iteration 787 is 2.30273746529\n",
      "loss in iteration 788 is 2.30217079717\n",
      "loss in iteration 789 is 2.30187411235\n",
      "loss in iteration 790 is 2.30360329477\n",
      "loss in iteration 791 is 2.30098627265\n",
      "loss in iteration 792 is 2.30232945704\n",
      "loss in iteration 793 is 2.30333590255\n",
      "loss in iteration 794 is 2.30267334916\n",
      "loss in iteration 795 is 2.30306600596\n",
      "loss in iteration 796 is 2.30285343806\n",
      "loss in iteration 797 is 2.30187547151\n",
      "loss in iteration 798 is 2.30373893915\n",
      "loss in iteration 799 is 2.30284834996\n",
      "loss in iteration 800 is 2.30274247673\n",
      "loss in iteration 801 is 2.30314399729\n",
      "loss in iteration 802 is 2.30219691179\n",
      "loss in iteration 803 is 2.30255929229\n",
      "loss in iteration 804 is 2.30284086312\n",
      "loss in iteration 805 is 2.30322296597\n",
      "loss in iteration 806 is 2.30313757821\n",
      "loss in iteration 807 is 2.30316427803\n",
      "loss in iteration 808 is 2.30295528285\n",
      "loss in iteration 809 is 2.30291987545\n",
      "loss in iteration 810 is 2.3021899511\n",
      "loss in iteration 811 is 2.30167114872\n",
      "loss in iteration 812 is 2.30237172636\n",
      "loss in iteration 813 is 2.30204851303\n",
      "loss in iteration 814 is 2.3014563921\n",
      "loss in iteration 815 is 2.30271095531\n",
      "loss in iteration 816 is 2.30214157124\n",
      "loss in iteration 817 is 2.3022573436\n",
      "loss in iteration 818 is 2.30294207052\n",
      "loss in iteration 819 is 2.30121746778\n",
      "loss in iteration 820 is 2.3023351659\n",
      "loss in iteration 821 is 2.30285727359\n",
      "loss in iteration 822 is 2.30302520894\n",
      "loss in iteration 823 is 2.30178008545\n",
      "loss in iteration 824 is 2.30301162427\n",
      "loss in iteration 825 is 2.3038812249\n",
      "loss in iteration 826 is 2.30268054971\n",
      "loss in iteration 827 is 2.30233824412\n",
      "loss in iteration 828 is 2.30241876049\n",
      "loss in iteration 829 is 2.30227245937\n",
      "loss in iteration 830 is 2.30225943336\n",
      "loss in iteration 831 is 2.30228019041\n",
      "loss in iteration 832 is 2.30239535083\n",
      "loss in iteration 833 is 2.30347366188\n",
      "loss in iteration 834 is 2.30166611762\n",
      "loss in iteration 835 is 2.30342947104\n",
      "loss in iteration 836 is 2.30151323174\n",
      "loss in iteration 837 is 2.30388258073\n",
      "loss in iteration 838 is 2.30259239194\n",
      "loss in iteration 839 is 2.30178697296\n",
      "loss in iteration 840 is 2.3031046969\n",
      "loss in iteration 841 is 2.30310198244\n",
      "loss in iteration 842 is 2.30257056826\n",
      "loss in iteration 843 is 2.30180905541\n",
      "loss in iteration 844 is 2.3028097512\n",
      "loss in iteration 845 is 2.30306029859\n",
      "loss in iteration 846 is 2.30328029094\n",
      "loss in iteration 847 is 2.30235804354\n",
      "loss in iteration 848 is 2.30296228428\n",
      "loss in iteration 849 is 2.30282061877\n",
      "loss in iteration 850 is 2.30240130792\n",
      "loss in iteration 851 is 2.30328961457\n",
      "loss in iteration 852 is 2.30215076064\n",
      "loss in iteration 853 is 2.30242569044\n",
      "loss in iteration 854 is 2.30302212084\n",
      "loss in iteration 855 is 2.30288869187\n",
      "loss in iteration 856 is 2.30106342238\n",
      "loss in iteration 857 is 2.30193727535\n",
      "loss in iteration 858 is 2.30366065557\n",
      "loss in iteration 859 is 2.30261317568\n",
      "loss in iteration 860 is 2.30170765563\n",
      "loss in iteration 861 is 2.3020409863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration 862 is 2.30234080732\n",
      "loss in iteration 863 is 2.30131036522\n",
      "loss in iteration 864 is 2.30186576697\n",
      "loss in iteration 865 is 2.30267211482\n",
      "loss in iteration 866 is 2.30298810205\n",
      "loss in iteration 867 is 2.30156848004\n",
      "loss in iteration 868 is 2.30218611301\n",
      "loss in iteration 869 is 2.30184988066\n",
      "loss in iteration 870 is 2.30291402959\n",
      "loss in iteration 871 is 2.30265218033\n",
      "loss in iteration 872 is 2.30247625501\n",
      "loss in iteration 873 is 2.3024024844\n",
      "loss in iteration 874 is 2.30334061915\n",
      "loss in iteration 875 is 2.30237876689\n",
      "loss in iteration 876 is 2.30226295215\n",
      "loss in iteration 877 is 2.30309003724\n",
      "loss in iteration 878 is 2.3033359189\n",
      "loss in iteration 879 is 2.30273087926\n",
      "loss in iteration 880 is 2.30219735793\n",
      "loss in iteration 881 is 2.3028166581\n",
      "loss in iteration 882 is 2.30239517002\n",
      "loss in iteration 883 is 2.30329818989\n",
      "loss in iteration 884 is 2.30258053103\n",
      "loss in iteration 885 is 2.30341223553\n",
      "loss in iteration 886 is 2.30304584681\n",
      "loss in iteration 887 is 2.30238998722\n",
      "loss in iteration 888 is 2.30346124798\n",
      "loss in iteration 889 is 2.30228740361\n",
      "loss in iteration 890 is 2.30260450923\n",
      "loss in iteration 891 is 2.30268636908\n",
      "loss in iteration 892 is 2.30204292131\n",
      "loss in iteration 893 is 2.30115360654\n",
      "loss in iteration 894 is 2.30198553515\n",
      "loss in iteration 895 is 2.30242886204\n",
      "loss in iteration 896 is 2.30220732227\n",
      "loss in iteration 897 is 2.30278197108\n",
      "loss in iteration 898 is 2.30331438046\n",
      "loss in iteration 899 is 2.30285083038\n",
      "loss in iteration 900 is 2.30352484306\n",
      "loss in iteration 901 is 2.30302657169\n",
      "loss in iteration 902 is 2.30258434851\n",
      "loss in iteration 903 is 2.30229384465\n",
      "loss in iteration 904 is 2.30205028818\n",
      "loss in iteration 905 is 2.30262256635\n",
      "loss in iteration 906 is 2.30274006863\n",
      "loss in iteration 907 is 2.30331530955\n",
      "loss in iteration 908 is 2.30239780819\n",
      "loss in iteration 909 is 2.30235925397\n",
      "loss in iteration 910 is 2.30133731264\n",
      "loss in iteration 911 is 2.30228832636\n",
      "loss in iteration 912 is 2.30212866122\n",
      "loss in iteration 913 is 2.30277091937\n",
      "loss in iteration 914 is 2.30287019753\n",
      "loss in iteration 915 is 2.30182857147\n",
      "loss in iteration 916 is 2.30206919269\n",
      "loss in iteration 917 is 2.30242552148\n",
      "loss in iteration 918 is 2.3030562908\n",
      "loss in iteration 919 is 2.30218340494\n",
      "loss in iteration 920 is 2.30165701283\n",
      "loss in iteration 921 is 2.30278396569\n",
      "loss in iteration 922 is 2.30264338648\n",
      "loss in iteration 923 is 2.30251008543\n",
      "loss in iteration 924 is 2.30278284995\n",
      "loss in iteration 925 is 2.30255483566\n",
      "loss in iteration 926 is 2.30274267195\n",
      "loss in iteration 927 is 2.30221493354\n",
      "loss in iteration 928 is 2.3030356478\n",
      "loss in iteration 929 is 2.30201860567\n",
      "loss in iteration 930 is 2.30306443152\n",
      "loss in iteration 931 is 2.30249315685\n",
      "loss in iteration 932 is 2.30248755337\n",
      "loss in iteration 933 is 2.30273135378\n",
      "loss in iteration 934 is 2.30270415176\n",
      "loss in iteration 935 is 2.30336250715\n",
      "loss in iteration 936 is 2.30223669279\n",
      "loss in iteration 937 is 2.30338847023\n",
      "loss in iteration 938 is 2.30301270323\n",
      "loss in iteration 939 is 2.30232755312\n",
      "loss in iteration 940 is 2.30216004578\n",
      "loss in iteration 941 is 2.30296065267\n",
      "loss in iteration 942 is 2.30153380765\n",
      "loss in iteration 943 is 2.30271130657\n",
      "loss in iteration 944 is 2.30278429497\n",
      "loss in iteration 945 is 2.30327119992\n",
      "loss in iteration 946 is 2.30278928864\n",
      "loss in iteration 947 is 2.3018378668\n",
      "loss in iteration 948 is 2.30289320674\n",
      "loss in iteration 949 is 2.30278613548\n",
      "loss in iteration 950 is 2.30326661356\n",
      "loss in iteration 951 is 2.30238295562\n",
      "loss in iteration 952 is 2.30198441453\n",
      "loss in iteration 953 is 2.30280136708\n",
      "loss in iteration 954 is 2.30221856845\n",
      "loss in iteration 955 is 2.30231194168\n",
      "loss in iteration 956 is 2.30172519154\n",
      "loss in iteration 957 is 2.30305599437\n",
      "loss in iteration 958 is 2.30141499103\n",
      "loss in iteration 959 is 2.30274479265\n",
      "loss in iteration 960 is 2.3021234717\n",
      "loss in iteration 961 is 2.30300961891\n",
      "loss in iteration 962 is 2.30298632927\n",
      "loss in iteration 963 is 2.30183439279\n",
      "loss in iteration 964 is 2.30187984538\n",
      "loss in iteration 965 is 2.30219877174\n",
      "loss in iteration 966 is 2.30213775035\n",
      "loss in iteration 967 is 2.30192531731\n",
      "loss in iteration 968 is 2.30296877805\n",
      "loss in iteration 969 is 2.30278885862\n",
      "loss in iteration 970 is 2.30225489038\n",
      "loss in iteration 971 is 2.30293933629\n",
      "loss in iteration 972 is 2.30239626745\n",
      "loss in iteration 973 is 2.3025920993\n",
      "loss in iteration 974 is 2.3025054888\n",
      "loss in iteration 975 is 2.30185137304\n",
      "loss in iteration 976 is 2.3014679038\n",
      "loss in iteration 977 is 2.3031818926\n",
      "loss in iteration 978 is 2.30184655983\n",
      "loss in iteration 979 is 2.30209370914\n",
      "loss in iteration 980 is 2.30192743972\n",
      "loss in iteration 981 is 2.30175941436\n",
      "loss in iteration 982 is 2.30303360228\n",
      "loss in iteration 983 is 2.30294026137\n",
      "loss in iteration 984 is 2.3035555646\n",
      "loss in iteration 985 is 2.30252018461\n",
      "loss in iteration 986 is 2.30324259398\n",
      "loss in iteration 987 is 2.30294778\n",
      "loss in iteration 988 is 2.30313504763\n",
      "loss in iteration 989 is 2.30148830749\n",
      "loss in iteration 990 is 2.30247212636\n",
      "loss in iteration 991 is 2.30253210013\n",
      "loss in iteration 992 is 2.30246324933\n",
      "loss in iteration 993 is 2.30193952624\n",
      "loss in iteration 994 is 2.30266394301\n",
      "loss in iteration 995 is 2.30218313425\n",
      "loss in iteration 996 is 2.30192264564\n",
      "loss in iteration 997 is 2.30191300568\n",
      "loss in iteration 998 is 2.30356845226\n",
      "loss in iteration 999 is 2.30366075756\n",
      "loss in iteration 1000 is 2.30289707359\n",
      "loss in iteration 1001 is 2.3034494777\n",
      "loss in iteration 1002 is 2.30240441102\n",
      "loss in iteration 1003 is 2.30242678709\n",
      "loss in iteration 1004 is 2.30274778819\n",
      "loss in iteration 1005 is 2.3026861525\n",
      "loss in iteration 1006 is 2.30343409578\n",
      "loss in iteration 1007 is 2.30310233902\n",
      "loss in iteration 1008 is 2.30186521568\n",
      "loss in iteration 1009 is 2.30203787029\n",
      "loss in iteration 1010 is 2.30340196925\n",
      "loss in iteration 1011 is 2.30117152841\n",
      "loss in iteration 1012 is 2.30255076869\n",
      "loss in iteration 1013 is 2.30318104588\n",
      "loss in iteration 1014 is 2.3021476119\n",
      "loss in iteration 1015 is 2.30205364377\n",
      "loss in iteration 1016 is 2.30291017591\n",
      "loss in iteration 1017 is 2.3023985828\n",
      "loss in iteration 1018 is 2.30203387185\n",
      "loss in iteration 1019 is 2.30183010858\n",
      "loss in iteration 1020 is 2.30219372238\n",
      "loss in iteration 1021 is 2.30312631535\n",
      "loss in iteration 1022 is 2.30221758159\n",
      "loss in iteration 1023 is 2.30244968332\n",
      "loss in iteration 1024 is 2.30264809963\n",
      "loss in iteration 1025 is 2.30312298071\n",
      "loss in iteration 1026 is 2.30300166875\n",
      "loss in iteration 1027 is 2.30190691553\n",
      "loss in iteration 1028 is 2.3019315002\n",
      "loss in iteration 1029 is 2.3020361685\n",
      "loss in iteration 1030 is 2.30297447819\n",
      "loss in iteration 1031 is 2.30185794706\n",
      "loss in iteration 1032 is 2.30247755311\n",
      "loss in iteration 1033 is 2.30208428516\n",
      "loss in iteration 1034 is 2.30107444247\n",
      "loss in iteration 1035 is 2.30188916807\n",
      "loss in iteration 1036 is 2.30233382951\n",
      "loss in iteration 1037 is 2.30206145268\n",
      "loss in iteration 1038 is 2.30250312539\n",
      "loss in iteration 1039 is 2.30282501111\n",
      "loss in iteration 1040 is 2.30261458155\n",
      "loss in iteration 1041 is 2.30328688017\n",
      "loss in iteration 1042 is 2.30231985022\n",
      "loss in iteration 1043 is 2.30215162957\n",
      "loss in iteration 1044 is 2.30273345038\n",
      "loss in iteration 1045 is 2.30261214929\n",
      "loss in iteration 1046 is 2.30350758979\n",
      "loss in iteration 1047 is 2.30246488293\n",
      "loss in iteration 1048 is 2.30204127345\n",
      "loss in iteration 1049 is 2.3028072203\n",
      "loss in iteration 1050 is 2.3021849789\n",
      "loss in iteration 1051 is 2.30168712935\n",
      "loss in iteration 1052 is 2.30206857094\n",
      "loss in iteration 1053 is 2.30235556396\n",
      "loss in iteration 1054 is 2.30342636454\n",
      "loss in iteration 1055 is 2.30296576212\n",
      "loss in iteration 1056 is 2.30299898897\n",
      "loss in iteration 1057 is 2.30278243996\n",
      "loss in iteration 1058 is 2.30285172034\n",
      "loss in iteration 1059 is 2.30349319113\n",
      "loss in iteration 1060 is 2.30293375905\n",
      "loss in iteration 1061 is 2.30281389943\n",
      "loss in iteration 1062 is 2.30190966269\n",
      "loss in iteration 1063 is 2.30246720274\n",
      "loss in iteration 1064 is 2.30320220427\n",
      "loss in iteration 1065 is 2.30319291434\n",
      "loss in iteration 1066 is 2.30242841432\n",
      "loss in iteration 1067 is 2.30254324017\n",
      "loss in iteration 1068 is 2.30217885059\n",
      "loss in iteration 1069 is 2.30324187033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss in iteration 1070 is 2.30164162695\n",
      "loss in iteration 1071 is 2.30226133816\n",
      "loss in iteration 1072 is 2.30217671034\n",
      "loss in iteration 1073 is 2.30286502485\n",
      "loss in iteration 1074 is 2.3030594676\n",
      "loss in iteration 1075 is 2.30170555784\n",
      "loss in iteration 1076 is 2.30332914469\n",
      "loss in iteration 1077 is 2.3035042686\n",
      "loss in iteration 1078 is 2.30151900579\n",
      "loss in iteration 1079 is 2.3018917721\n",
      "loss in iteration 1080 is 2.30198846375\n",
      "loss in iteration 1081 is 2.30199762587\n",
      "loss in iteration 1082 is 2.30276385055\n",
      "loss in iteration 1083 is 2.30318661547\n",
      "loss in iteration 1084 is 2.30276460038\n",
      "loss in iteration 1085 is 2.30263306666\n",
      "loss in iteration 1086 is 2.30290151531\n",
      "loss in iteration 1087 is 2.3020985876\n",
      "loss in iteration 1088 is 2.3031894904\n",
      "loss in iteration 1089 is 2.30215804402\n",
      "loss in iteration 1090 is 2.30159495067\n",
      "loss in iteration 1091 is 2.30159176349\n",
      "loss in iteration 1092 is 2.30265369955\n",
      "loss in iteration 1093 is 2.30287100696\n",
      "loss in iteration 1094 is 2.30227474208\n",
      "loss in iteration 1095 is 2.30299307749\n",
      "loss in iteration 1096 is 2.30360287134\n",
      "loss in iteration 1097 is 2.3029412332\n",
      "loss in iteration 1098 is 2.30266917976\n",
      "loss in iteration 1099 is 2.30200081893\n",
      "loss in iteration 1100 is 2.30262877336\n",
      "loss in iteration 1101 is 2.30197585012\n",
      "loss in iteration 1102 is 2.30187268403\n",
      "loss in iteration 1103 is 2.30257701732\n",
      "loss in iteration 1104 is 2.30153644063\n",
      "loss in iteration 1105 is 2.30283563367\n",
      "loss in iteration 1106 is 2.30313079827\n",
      "loss in iteration 1107 is 2.30176703368\n",
      "loss in iteration 1108 is 2.30210677521\n",
      "loss in iteration 1109 is 2.30152278933\n",
      "loss in iteration 1110 is 2.30318784065\n",
      "loss in iteration 1111 is 2.30321226952\n",
      "loss in iteration 1112 is 2.30283232167\n",
      "loss in iteration 1113 is 2.30251631449\n",
      "loss in iteration 1114 is 2.30257518554\n",
      "loss in iteration 1115 is 2.30109430165\n",
      "loss in iteration 1116 is 2.30200120693\n",
      "loss in iteration 1117 is 2.30238113539\n",
      "loss in iteration 1118 is 2.30346973129\n",
      "loss in iteration 1119 is 2.30246206756\n",
      "loss in iteration 1120 is 2.3028593388\n",
      "loss in iteration 1121 is 2.30258435887\n",
      "loss in iteration 1122 is 2.30230766374\n",
      "loss in iteration 1123 is 2.30149787436\n",
      "loss in iteration 1124 is 2.30280936523\n",
      "loss in iteration 1125 is 2.30319452248\n",
      "loss in iteration 1126 is 2.30303871659\n",
      "loss in iteration 1127 is 2.30262974335\n",
      "loss in iteration 1128 is 2.30281366909\n",
      "loss in iteration 1129 is 2.30230631779\n",
      "loss in iteration 1130 is 2.30211203507\n",
      "loss in iteration 1131 is 2.30240298139\n",
      "loss in iteration 1132 is 2.30283094733\n",
      "loss in iteration 1133 is 2.30175168581\n",
      "loss in iteration 1134 is 2.30267675821\n",
      "loss in iteration 1135 is 2.30265907187\n",
      "loss in iteration 1136 is 2.30170356709\n",
      "loss in iteration 1137 is 2.30131414082\n",
      "loss in iteration 1138 is 2.30245458511\n",
      "loss in iteration 1139 is 2.30341596728\n",
      "loss in iteration 1140 is 2.30298405828\n",
      "loss in iteration 1141 is 2.30247938907\n",
      "loss in iteration 1142 is 2.30383946937\n",
      "loss in iteration 1143 is 2.30126080511\n",
      "loss in iteration 1144 is 2.30323939513\n",
      "loss in iteration 1145 is 2.30245158818\n",
      "loss in iteration 1146 is 2.30250560001\n",
      "loss in iteration 1147 is 2.30280868309\n",
      "loss in iteration 1148 is 2.30240613259\n",
      "loss in iteration 1149 is 2.30244220532\n",
      "loss in iteration 1150 is 2.30257987655\n",
      "loss in iteration 1151 is 2.3025221288\n",
      "loss in iteration 1152 is 2.30233719172\n",
      "loss in iteration 1153 is 2.30399925255\n",
      "loss in iteration 1154 is 2.30211697135\n",
      "loss in iteration 1155 is 2.30238508207\n",
      "loss in iteration 1156 is 2.30243545405\n",
      "loss in iteration 1157 is 2.30203938439\n",
      "loss in iteration 1158 is 2.30146377101\n",
      "loss in iteration 1159 is 2.30282094567\n",
      "loss in iteration 1160 is 2.30368847902\n",
      "loss in iteration 1161 is 2.30286150177\n",
      "loss in iteration 1162 is 2.30224024154\n",
      "loss in iteration 1163 is 2.30283868167\n",
      "loss in iteration 1164 is 2.30251781083\n",
      "loss in iteration 1165 is 2.30197824805\n",
      "loss in iteration 1166 is 2.30268080198\n"
     ]
    }
   ],
   "source": [
    "layer_dimensions = [3072,20,20,20,10]  # including the input and output layers  \n",
    "# 3072 is the input feature size, 10 is the number of outputs in the final layer\n",
    "FCN = FullyConnectedNetwork(layer_dimensions, lambd=0)\n",
    "FCN.train(X_train, Y_train, max_iters=10000, batch_size=200, learning_rate=0.0001,validate_every=200)\n",
    "# lambd, the L2 regularization penalty hyperparamter will be 0 for this part\n",
    "y_predicted = FCN.evaluate(X_test,Y_test)  # print accuracy on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dimensions = [3072,..,.., 10]  # including the input and output layers  \n",
    "# 3072 is the input feature size, 10 is the number of outputs in the final layer\n",
    "FCN = FullyConnectedNetwork(layer_dimensions, lambd=0.1)\n",
    "FCN.train(X_train, Y_train, max_iters=10000, batch_size=200, learning_rate=0.0001, validate_every=200)\n",
    "# lambd, the L2 regularization penalty hyperparamter will not be 0 for this part\n",
    "y_predicted = FCN.evaluate(X_test)  # print accuracy on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
