{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    \n",
    "    def unpickle(self,file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "    \n",
    "    def load_train_data(self):\n",
    "        '''\n",
    "        loads training data: 50,000 examples with 3072 features\n",
    "        '''\n",
    "        X_train = None\n",
    "        Y_train = None\n",
    "        for i in range(1, 6):\n",
    "            pickleFile = self.unpickle('cifar-10-batches-py/data_batch_{}'.format(i))\n",
    "            dataX = pickleFile[b'data']\n",
    "            dataY = pickleFile[b'labels']\n",
    "            if type(X_train) is np.ndarray:\n",
    "                X_train = np.concatenate((X_train, dataX))\n",
    "                Y_train = np.concatenate((Y_train, dataY))\n",
    "            else:\n",
    "                X_train = dataX\n",
    "                Y_train = dataY\n",
    "\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "\n",
    "        return X_train, Y_train\n",
    "\n",
    "    def load_test_data(self):\n",
    "        '''\n",
    "        loads testing data: 10,000 examples with 3072 features\n",
    "        '''\n",
    "        X_test = None\n",
    "        Y_test = None\n",
    "        pickleFile = self.unpickle('cifar-10-batches-py/test_batch')\n",
    "        dataX = pickleFile[b'data']\n",
    "        dataY = pickleFile[b'labels']\n",
    "        if type(X_test) is np.ndarray:\n",
    "            X_test = np.concatenate((X_test, dataX))\n",
    "            Y_test = np.concatenate((Y_test, dataY))\n",
    "        else:\n",
    "            X_test = np.array(dataX)\n",
    "            Y_test = np.array(dataY)\n",
    "\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "        return X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "X_trainval,Y_trainval = Loader().load_train_data()\n",
    "X_test, Y_test = Loader().load_test_data()\n",
    "\n",
    "X_trainval = X_trainval.astype(float)\n",
    "X_trainval_scale = X_trainval / 255\n",
    "\n",
    "X_test = X_test.astype(float)\n",
    "X_test_scale = X_test / 255\n",
    "\n",
    "X_trainval_reshape = X_trainval_scale.reshape((X_trainval.shape[0],3,32,32))\n",
    "X_test_reshape = X_test_scale.reshape((X_test.shape[0],3,32,32))\n",
    "\n",
    "\n",
    "num_train = 50000\n",
    "indices = list(range(num_train))\n",
    "split = 40000\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "valid_idx, train_idx = indices[split:], indices[:split]\n",
    "\n",
    "X_train_reshape = X_trainval_reshape[train_idx]\n",
    "Y_train_reshape = Y_trainval[train_idx]\n",
    "\n",
    "X_val_reshape = X_trainval_reshape[valid_idx]\n",
    "Y_val_reshape = Y_trainval[valid_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_reshape.reshape(40000).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(X_train_reshape.shape)\n",
    "print(Y_train_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "\n",
    "train_try = data_utils.TensorDataset(torch.FloatTensor(X_train_reshape), torch.LongTensor(Y_train_reshape.reshape(40000)))\n",
    "train_try_loader = data_utils.DataLoader(train_try, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_try_loader, 0):\n",
    "    # get the inputs\n",
    "    inputs, labels = data\n",
    "    print(inputs)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 3,padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 32, 3,padding=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(32,64,3,padding=1)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(64*4*4,120)\n",
    "        self.fc2 = nn.Linear(120,96)\n",
    "        self.fc3 = nn.Linear(96,10)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(10)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.drop = nn.Dropout2d(p=0.7)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        #x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        #x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        \n",
    "        \n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        #x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        #x = self.pool(F.relu(self.conv4(x)))\n",
    "        #print(x)\n",
    "\n",
    "        \n",
    "        x = x.view(-1,64*4*4)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        #x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "\n",
    "import torch.optim as optim\n",
    "from random import randint\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 2.090 train_acc: 0.215 val_acc: 0.360 learning rate: 0.001000\n",
      "epoch: 1 loss: 1.518 train_acc: 0.449 val_acc: 0.500 learning rate: 0.001000\n",
      "epoch: 2 loss: 1.244 train_acc: 0.557 val_acc: 0.573 learning rate: 0.001000\n",
      "epoch: 3 loss: 1.071 train_acc: 0.617 val_acc: 0.627 learning rate: 0.001000\n",
      "epoch: 4 loss: 0.961 train_acc: 0.660 val_acc: 0.623 learning rate: 0.001000\n",
      "epoch: 5 loss: 0.868 train_acc: 0.695 val_acc: 0.637 learning rate: 0.001000\n",
      "epoch: 6 loss: 0.794 train_acc: 0.718 val_acc: 0.651 learning rate: 0.001000\n",
      "epoch: 7 loss: 0.723 train_acc: 0.744 val_acc: 0.672 learning rate: 0.001000\n",
      "epoch: 8 loss: 0.662 train_acc: 0.764 val_acc: 0.664 learning rate: 0.001000\n",
      "epoch: 9 loss: 0.606 train_acc: 0.785 val_acc: 0.659 learning rate: 0.001000\n",
      "epoch: 10 loss: 0.366 train_acc: 0.872 val_acc: 0.709 learning rate: 0.000250\n",
      "epoch: 11 loss: 0.294 train_acc: 0.897 val_acc: 0.705 learning rate: 0.000250\n",
      "epoch: 12 loss: 0.246 train_acc: 0.915 val_acc: 0.706 learning rate: 0.000250\n",
      "epoch: 13 loss: 0.205 train_acc: 0.931 val_acc: 0.702 learning rate: 0.000250\n",
      "epoch: 14 loss: 0.170 train_acc: 0.944 val_acc: 0.702 learning rate: 0.000250\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "temp_lr = 0.001\n",
    "prev_acc = 0\n",
    "\n",
    "\n",
    "best_net = Net()\n",
    "train_loss = []\n",
    "validation_acc = []\n",
    "best_acc = 0\n",
    "for epoch in range(15):  # loop over the dataset multiple times\n",
    "    if epoch > 0:\n",
    "        #if (prev_acc / acc > 0.995):\n",
    "        #    temp_lr /= 5\n",
    "        #prev_acc = acc\n",
    "        if (epoch % 10 == 0):\n",
    "            temp_lr /= 4\n",
    "    optimizer = optim.SGD(net.parameters(), lr=temp_lr, momentum=0.9)\n",
    "    #temp_lr /= 5\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_try_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        _, predicted_train = torch.max(outputs.data, 1)\n",
    "        \n",
    "        for i in range(4):\n",
    "            if predicted_train[i] == int(targets[i].data.numpy()[0]):\n",
    "                train_correct += 1\n",
    "            train_total +=1\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        #print(\"Summary: %d is %.3f\" % (epoch, acc))\n",
    "        \n",
    "    train_acc = train_correct / train_total\n",
    "        \n",
    "        \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(0,10000):\n",
    "        inputs = torch.FloatTensor(X_val_reshape[i].reshape((1,3,32,32)))\n",
    "        labels = torch.LongTensor(Y_val_reshape[i])\n",
    "        outputs = net(Variable(inputs))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        if predicted[0] == labels[0]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "    acc = correct / total\n",
    "    train_loss.append(running_loss/10000)\n",
    "    validation_acc.append(acc)\n",
    "    if (acc > best_acc):\n",
    "        best_acc = acc\n",
    "        best_net.load_state_dict(net.state_dict())\n",
    "    \n",
    "    print(\"epoch: %d loss: %.3f train_acc: %.3f val_acc: %.3f learning rate: %.6f\" % (epoch,running_loss/10000,train_acc,acc,temp_lr))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7062\n",
      "10000\n",
      "0.7062\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for i in range(0,10000):\n",
    "    inputs = torch.FloatTensor(X_test_reshape[i].reshape((1,3,32,32)))\n",
    "    labels = torch.LongTensor(Y_test[i])\n",
    "    outputs = best_net(Variable(inputs))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    if predicted[0] == labels[0]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(correct)\n",
    "print(total)\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0899593315303324,\n",
       " 1.5176347495943308,\n",
       " 1.2436639948002994,\n",
       " 1.0712805995244532,\n",
       " 0.9608085837502033,\n",
       " 0.8682140761709772,\n",
       " 0.7941201634490047,\n",
       " 0.7226411649866729,\n",
       " 0.6621319993035868,\n",
       " 0.6055460957959993,\n",
       " 0.36587994550366676,\n",
       " 0.2937783408456708,\n",
       " 0.24561901038416867,\n",
       " 0.20516464548483682,\n",
       " 0.16961843757615463]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3603,\n",
       " 0.5003,\n",
       " 0.5733,\n",
       " 0.627,\n",
       " 0.6227,\n",
       " 0.6368,\n",
       " 0.6512,\n",
       " 0.672,\n",
       " 0.6645,\n",
       " 0.6589,\n",
       " 0.7091,\n",
       " 0.705,\n",
       " 0.7055,\n",
       " 0.7016,\n",
       " 0.7022]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
