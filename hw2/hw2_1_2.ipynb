{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    \n",
    "    def unpickle(self,file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "    \n",
    "    def load_train_data(self):\n",
    "        '''\n",
    "        loads training data: 50,000 examples with 3072 features\n",
    "        '''\n",
    "        X_train = None\n",
    "        Y_train = None\n",
    "        for i in range(1, 6):\n",
    "            pickleFile = self.unpickle('cifar-10-batches-py/data_batch_{}'.format(i))\n",
    "            dataX = pickleFile[b'data']\n",
    "            dataY = pickleFile[b'labels']\n",
    "            if type(X_train) is np.ndarray:\n",
    "                X_train = np.concatenate((X_train, dataX))\n",
    "                Y_train = np.concatenate((Y_train, dataY))\n",
    "            else:\n",
    "                X_train = dataX\n",
    "                Y_train = dataY\n",
    "\n",
    "        Y_train = Y_train.reshape(Y_train.shape[0], 1)\n",
    "\n",
    "        return X_train, Y_train\n",
    "\n",
    "    def load_test_data(self):\n",
    "        '''\n",
    "        loads testing data: 10,000 examples with 3072 features\n",
    "        '''\n",
    "        X_test = None\n",
    "        Y_test = None\n",
    "        pickleFile = self.unpickle('cifar-10-batches-py/test_batch')\n",
    "        dataX = pickleFile[b'data']\n",
    "        dataY = pickleFile[b'labels']\n",
    "        if type(X_test) is np.ndarray:\n",
    "            X_test = np.concatenate((X_test, dataX))\n",
    "            Y_test = np.concatenate((Y_test, dataY))\n",
    "        else:\n",
    "            X_test = np.array(dataX)\n",
    "            Y_test = np.array(dataY)\n",
    "\n",
    "        Y_test = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "        return X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "X_trainval,Y_trainval = Loader().load_train_data()\n",
    "X_test, Y_test = Loader().load_test_data()\n",
    "\n",
    "X_trainval = X_trainval.astype(float)\n",
    "X_trainval_scale = X_trainval / 255\n",
    "\n",
    "X_test = X_test.astype(float)\n",
    "X_test_scale = X_test / 255\n",
    "\n",
    "X_trainval_reshape = X_trainval_scale.reshape((X_trainval.shape[0],3,32,32))\n",
    "X_test_reshape = X_test_scale.reshape((X_test.shape[0],3,32,32))\n",
    "\n",
    "\n",
    "num_train = 50000\n",
    "indices = list(range(num_train))\n",
    "split = 40000\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "valid_idx, train_idx = indices[split:], indices[:split]\n",
    "\n",
    "X_train_reshape = X_trainval_reshape[train_idx]\n",
    "Y_train_reshape = Y_trainval[train_idx]\n",
    "\n",
    "X_val_reshape = X_trainval_reshape[valid_idx]\n",
    "Y_val_reshape = Y_trainval[valid_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data_utils\n",
    "\n",
    "train_try = data_utils.TensorDataset(torch.FloatTensor(X_train_reshape), torch.LongTensor(Y_train_reshape.reshape(40000)))\n",
    "train_try_loader = data_utils.DataLoader(train_try, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 120, 5,padding=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(120, 42, 3,padding=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(42,96,3,padding=2)\n",
    "        \n",
    "        #self.conv4 = nn.Conv2d(72,168,3,padding=2)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(96*5*5,120)\n",
    "        self.fc2 = nn.Linear(120,96)\n",
    "        self.fc3 = nn.Linear(96,10)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(120)\n",
    "        self.bn2 = nn.BatchNorm2d(42)\n",
    "        self.bn3 = nn.BatchNorm2d(96)\n",
    "        self.bn4 = nn.BatchNorm2d(168)\n",
    "        \n",
    "        self.drop = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(self.conv1(x).shape)\n",
    "        x = self.pool(self.bn1(F.relu(self.conv1(x))))\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = self.pool(self.bn2(F.relu(self.conv2(x))))\n",
    "        #x = self.drop(self.pool(F.relu(self.conv2(x))))\n",
    "        \n",
    "        x = self.pool(self.bn3(F.relu(self.conv3(x))))\n",
    "        \n",
    "        #x = self.pool(self.bn4(F.relu(self.conv4(x))))\n",
    "        #x = self.drop(self.pool(F.relu(self.conv3(x))))\n",
    "        #print(x.shape)\n",
    "        \n",
    "        \n",
    "        x = x.view(-1,96*5*5)\n",
    "\n",
    "        x = (F.relu(self.fc1(x)))\n",
    "        x = (F.relu(self.fc2(x)))\n",
    "        x = (self.fc3(x))\n",
    "        \n",
    "        #x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()\n",
    "\n",
    "import torch.optim as optim\n",
    "from random import randint\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9,weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 1.362 train_acc: 0.509 val_acc: 0.577 learning rate: 0.001000\n",
      "epoch: 1 loss: 0.976 train_acc: 0.658 val_acc: 0.636 learning rate: 0.001000\n",
      "epoch: 2 loss: 0.808 train_acc: 0.717 val_acc: 0.686 learning rate: 0.001000\n",
      "epoch: 3 loss: 0.693 train_acc: 0.760 val_acc: 0.686 learning rate: 0.001000\n",
      "epoch: 4 loss: 0.446 train_acc: 0.844 val_acc: 0.728 learning rate: 0.000200\n",
      "epoch: 5 loss: 0.362 train_acc: 0.874 val_acc: 0.726 learning rate: 0.000200\n",
      "epoch: 6 loss: 0.276 train_acc: 0.909 val_acc: 0.735 learning rate: 0.000040\n",
      "epoch: 7 loss: 0.255 train_acc: 0.916 val_acc: 0.735 learning rate: 0.000040\n",
      "epoch: 8 loss: 0.236 train_acc: 0.924 val_acc: 0.735 learning rate: 0.000008\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-74eb6b953657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "temp_lr = 0.001\n",
    "prev_acc = 0\n",
    "\n",
    "\n",
    "best_net = Net()\n",
    "train_loss = []\n",
    "validation_acc = []\n",
    "best_acc = 0\n",
    "for epoch in range(12):  # loop over the dataset multiple times\n",
    "    if epoch > 0:\n",
    "        if (prev_acc / acc > 0.995):\n",
    "            temp_lr /= 5\n",
    "        prev_acc = acc\n",
    "        #if (epoch % 9 == 0):\n",
    "        #    temp_lr /= 4\n",
    "        #if (epoch % 12 == 0):\n",
    "        #    temp_lr /=4\n",
    "    optimizer = optim.SGD(net.parameters(), lr=temp_lr, momentum=0.9,weight_decay=0.0005)\n",
    "    #temp_lr /= 5\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_try_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        _, predicted_train = torch.max(outputs.data, 1)\n",
    "        \n",
    "        for i in range(4):\n",
    "            if predicted_train[i] == int(targets[i].data.numpy()[0]):\n",
    "                train_correct += 1\n",
    "            train_total +=1\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        #print(\"Summary: %d is %.3f\" % (epoch, acc))\n",
    "        \n",
    "    train_acc = train_correct / train_total\n",
    "        \n",
    "        \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(0,10000):\n",
    "        inputs = torch.FloatTensor(X_val_reshape[i].reshape((1,3,32,32)))\n",
    "        labels = torch.LongTensor(Y_val_reshape[i])\n",
    "        outputs = net(Variable(inputs))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        if predicted[0] == labels[0]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "    acc = correct / total\n",
    "    train_loss.append(running_loss/10000)\n",
    "    validation_acc.append(acc)\n",
    "    if (acc > best_acc):\n",
    "        best_acc = acc\n",
    "        best_net.load_state_dict(net.state_dict())\n",
    "    \n",
    "    print(\"epoch: %d loss: %.3f train_acc: %.3f val_acc: %.3f learning rate: %.6f\" % (epoch,running_loss/10000,train_acc,acc,temp_lr))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7370\n",
      "10000\n",
      "0.737\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for i in range(0,10000):\n",
    "    inputs = torch.FloatTensor(X_test_reshape[i].reshape((1,3,32,32)))\n",
    "    labels = torch.LongTensor(Y_test[i])\n",
    "    outputs = best_net(Variable(inputs))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    if predicted[0] == labels[0]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(correct)\n",
    "print(total)\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.3620648444812744,\n",
       " 0.9759569225894287,\n",
       " 0.8083284358321922,\n",
       " 0.6927070099038654,\n",
       " 0.44611032965072084,\n",
       " 0.3616692439426306,\n",
       " 0.2763727409359948,\n",
       " 0.2553016137097882,\n",
       " 0.23622359152078187]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 192, 5,padding=2)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(192, 160, 3,padding=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(160,96,3,padding=2)\n",
    "        \n",
    "        #self.conv4 = nn.Conv2d(72,168,3,padding=2)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(96*5*5,120)\n",
    "        self.fc2 = nn.Linear(120,96)\n",
    "        self.fc3 = nn.Linear(96,10)\n",
    "\n",
    "        \n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(192)\n",
    "        self.bn2 = nn.BatchNorm2d(160)\n",
    "        self.bn3 = nn.BatchNorm2d(96)\n",
    "        #self.bn4 = nn.BatchNorm2d(168)\n",
    "        \n",
    "        self.drop = nn.Dropout2d(p=0.5)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(self.conv1(x).shape)\n",
    "        x = self.pool(self.bn1(F.relu(self.conv1(x))))\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = self.pool(self.bn2(F.relu(self.conv2(x))))\n",
    "        #x = self.drop(self.pool(F.relu(self.conv2(x))))\n",
    "        \n",
    "        x = self.pool(self.bn3(F.relu(self.conv3(x))))\n",
    "        \n",
    "        #x = self.pool(self.bn4(F.relu(self.conv4(x))))\n",
    "        #x = self.drop(self.pool(F.relu(self.conv3(x))))\n",
    "        #print(x.shape)\n",
    "        \n",
    "        \n",
    "        x = x.view(-1,96*5*5)\n",
    "\n",
    "        x = (F.relu(self.fc1(x)))\n",
    "        x = (F.relu(self.fc2(x)))\n",
    "        x = (self.fc3(x))\n",
    "        \n",
    "        #x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "net_2 = Net()\n",
    "\n",
    "import torch.optim as optim\n",
    "from random import randint\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net_2.parameters(), lr=0.001, momentum=0.9,weight_decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 1.329 train_acc: 0.521 val_acc: 0.607 learning rate: 0.001000\n",
      "epoch: 1 loss: 0.937 train_acc: 0.671 val_acc: 0.659 learning rate: 0.001000\n",
      "epoch: 2 loss: 0.768 train_acc: 0.730 val_acc: 0.695 learning rate: 0.001000\n",
      "epoch: 3 loss: 0.650 train_acc: 0.772 val_acc: 0.687 learning rate: 0.001000\n",
      "epoch: 4 loss: 0.392 train_acc: 0.866 val_acc: 0.744 learning rate: 0.000200\n",
      "epoch: 5 loss: 0.302 train_acc: 0.895 val_acc: 0.746 learning rate: 0.000200\n",
      "epoch: 6 loss: 0.219 train_acc: 0.928 val_acc: 0.750 learning rate: 0.000040\n",
      "epoch: 7 loss: 0.199 train_acc: 0.936 val_acc: 0.749 learning rate: 0.000040\n",
      "epoch: 8 loss: 0.179 train_acc: 0.943 val_acc: 0.750 learning rate: 0.000008\n",
      "epoch: 9 loss: 0.177 train_acc: 0.946 val_acc: 0.750 learning rate: 0.000002\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "temp_lr = 0.001\n",
    "prev_acc = 0\n",
    "\n",
    "\n",
    "best_net_2 = Net()\n",
    "train_loss_2 = []\n",
    "validation_acc_2 = []\n",
    "best_acc_2 = 0\n",
    "\n",
    "decrease = 0\n",
    "\n",
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "    if epoch > 0:\n",
    "        if (prev_acc / acc > 0.995):\n",
    "            temp_lr /= 5\n",
    "        prev_acc = acc\n",
    "        #prev_acc = acc\n",
    "        #if (epoch % 10 == 0):\n",
    "        #    temp_lr /= 4\n",
    "    optimizer = optim.SGD(net_2.parameters(), lr=temp_lr, momentum=0.9,weight_decay=0.0005)\n",
    "    #optimizer = optim.Adam(net_2.parameters(), lr=0.001)\n",
    "    #temp_lr /= 5\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_try_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net_2(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        _, predicted_train = torch.max(outputs.data, 1)\n",
    "        \n",
    "        for i in range(4):\n",
    "            if predicted_train[i] == int(targets[i].data.numpy()[0]):\n",
    "                train_correct += 1\n",
    "            train_total +=1\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        #print(\"Summary: %d is %.3f\" % (epoch, acc))\n",
    "        \n",
    "    train_acc = train_correct / train_total\n",
    "        \n",
    "        \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(0,10000):\n",
    "        inputs = torch.FloatTensor(X_val_reshape[i].reshape((1,3,32,32)))\n",
    "        labels = torch.LongTensor(Y_val_reshape[i])\n",
    "        outputs = net_2(Variable(inputs))\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        if predicted[0] == labels[0]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "    acc = correct / total\n",
    "    train_loss_2.append(running_loss/10000)\n",
    "    validation_acc_2.append(acc)\n",
    "    if (acc > best_acc_2):\n",
    "        best_acc_2 = acc\n",
    "        best_net_2.load_state_dict(net_2.state_dict())\n",
    "    \n",
    "    print(\"epoch: %d loss: %.3f train_acc: %.3f val_acc: %.3f learning rate: %.6f\" % (epoch,running_loss/10000,train_acc,acc,temp_lr))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7497\n",
      "10000\n",
      "0.7497\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for i in range(0,10000):\n",
    "    inputs = torch.FloatTensor(X_test_reshape[i].reshape((1,3,32,32)))\n",
    "    labels = torch.LongTensor(Y_test[i])\n",
    "    outputs = best_net_2(Variable(inputs))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    \n",
    "    if predicted[0] == labels[0]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(correct)\n",
    "print(total)\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
